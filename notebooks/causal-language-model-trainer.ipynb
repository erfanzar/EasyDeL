{
	"cells": [
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"# Training a Causal Language Model with EasyDeL\n",
				"In this tutorial, we will guide you through setting up and training a causal language model using the `EasyDeL` library. The example uses the Llama model architecture, demonstrating how to set up the model, prepare a training dataset, and configure the training process.\n",
				"\n",
				"----\n",
				"\n",
				"#### 1. Install Required Libraries\n",
				"First, let's install the necessary libraries and set up authentication for accessing the Hugging Face and Weights & Biases platforms."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"!pip install git+https://github.com/huggingface/transformers -U -q\n",
				"!pip install git+https://github.com/erfanzar/easydel.git -U -q\n",
				"!pip install jax[tpu] -f https://storage.googleapis.com/jax-releases/libtpu_releases.html -q -U\n",
				"\n",
				"# Configure git and login to Hugging Face Hub\n",
				"!git config --global credential.helper store\n",
				"!huggingface-cli login --token YOUR_HUGGINGFACE_TOKEN --add-to-git-credential\n",
				"\n",
				"# Login to Weights & Biases\n",
				"!wandb login YOUR_WANDB_API_KEY"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"Make sure to replace `YOUR_HUGGINGFACE_TOKEN` and `YOUR_WANDB_API_KEY` with your actual tokens. This step will set up the environment for training and allow you to track experiments.\n",
				"\n",
				"----\n",
				"\n",
				"#### 2. Import Required Libraries\n",
				"After installing the dependencies, import the necessary libraries for training."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"import easydel as ed\n",
				"from easydel.utils.analyze_memory import SMPMemoryMonitor  # Optional: For checking memory usage\n",
				"import jax\n",
				"from transformers import AutoTokenizer\n",
				"from jax import numpy as jnp, sharding, lax, random as jrnd\n",
				"from huggingface_hub import HfApi\n",
				"import datasets\n",
				"from flax.core import FrozenDict\n",
				"\n",
				"# Set up sharding and API utilities\n",
				"PartitionSpec, api = sharding.PartitionSpec, HfApi()\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"\n",
				"----\n",
				"\n",
				"#### 3. Model Configuration and Initialization\n",
				"Here, we define model parameters and load a pretrained Llama model using EasyDeL."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"sharding_axis_dims = (1, -1, 1, 1)\n",
				"max_length = 2048\n",
				"input_shape = (len(jax.devices()), max_length)\n",
				"pretrained_model_name_or_path = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
				"pretrained_model_name_or_path_tokenizer = pretrained_model_name_or_path\n",
				"new_repo_id = \"EasyDeL/Llama-3.2-3B-Instruct\"\n",
				"dtype = jnp.bfloat16\n",
				"\n",
				"# Load the pretrained model with automatic sharding\n",
				"model = ed.AutoEasyDeLModelForCausalLM.from_pretrained(\n",
				"    pretrained_model_name_or_path,\n",
				"    input_shape=input_shape,\n",
				"    auto_shard_model=True,\n",
				"    sharding_axis_dims=sharding_axis_dims,\n",
				"    config_kwargs=dict(\n",
				"        use_scan_mlp=False,\n",
				"        attn_dtype=jnp.float32,\n",
				"        freq_max_position_embeddings=max_length,\n",
				"        mask_max_position_embeddings=max_length,\n",
				"        attn_mechanism=ed.AttentionMechanisms.VANILLA\n",
				"    ),\n",
				"    param_dtype=dtype,\n",
				"    dtype=dtype,\n",
				"    precision=lax.Precision(\"fastest\"),\n",
				")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"This code initializes a Llama model with sharding for efficient multi-device training. Adjust the model name to load other pretrained models from the Hugging Face Hub.\n",
				"\n",
				"----\n",
				"\n",
				"#### 4. Prepare the Tokenizer\n",
				"We set up a tokenizer for the model, which is responsible for converting text into input IDs for training.\n",
				"\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"config = model.config\n",
				"model_use_tie_word_embedding = config.tie_word_embeddings\n",
				"model_parameters = FrozenDict({\"params\": params})\n",
				"\n",
				"# Initialize the tokenizer\n",
				"tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path_tokenizer, trust_remote_code=True)\n",
				"tokenizer.pad_token = tokenizer.eos_token if tokenizer.pad_token is None else tokenizer.pad_token\n",
				"tokenizer.padding_side = \"right\""
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"\n",
				"----\n",
				"\n",
				"#### 5. Load and Prepare the Dataset\n",
				"In this step, load your training data using the datasets library and apply the tokenizer."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Replace with your dataset loading code\n",
				"train_dataset = datasets.concatenate_datasets([\n",
				"    # Add datasets here\n",
				"])\n",
				"\n",
				"# Tokenize the dataset using a chat template function (adjust as needed)\n",
				"tokenized_dataset = train_dataset.map(\n",
				"    lambda x: tokenizer.apply_chat_template(x[\"conversation\"], tokenize=True, return_dict=True),\n",
				"    remove_columns=train_dataset.column_names\n",
				")\n",
				"\n",
				"# (Optional) Pack sequences to optimize training\n",
				"packed_dataset = ed.pack_sequences(tokenized_dataset, max_length)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"Make sure to replace the placeholder with your actual dataset details. You can explore different preprocessing methods depending on your dataset.\n",
				"\n",
				"----\n",
				"\n",
				"#### 6. Define Training Arguments\n",
				"Configure the training process by setting up various arguments such as learning rate, batch size, and training epochs."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"train_arguments = ed.TrainingArguments(\n",
				"    num_train_epochs=1,\n",
				"    learning_rate=9e-5,\n",
				"    learning_rate_end=9e-6,\n",
				"    warmup_steps=100,\n",
				"    optimizer=ed.EasyDeLOptimizers.ADAMW,\n",
				"    scheduler=ed.EasyDeLSchedulers.WARM_UP_COSINE,\n",
				"    weight_decay=0.02,\n",
				"    total_batch_size=48,\n",
				"    max_sequence_length=max_length,\n",
				"    gradient_checkpointing=ed.EasyDeLGradientCheckPointers.NOTHING_SAVEABLE,\n",
				"    sharding_array=sharding_axis_dims,\n",
				"    gradient_accumulation_steps=1,\n",
				"    init_input_shape=input_shape,\n",
				"    dtype=dtype,\n",
				"    param_dtype=dtype,\n",
				"    model_name=new_repo_id.split(\"/\")[-1].split(\"-v\")[0],\n",
				"    training_time=\"7H\",\n",
				"    track_memory=True, # Req go-lang\n",
				")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"The arguments can be adjusted based on your computational resources and training goals. Check the documentation for more details on each parameter.\n",
				"\n",
				"----\n",
				"\n",
				"#### 7. Train the Model\n",
				"Now, create the trainer and start training."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"trainer = ed.CausalLanguageModelTrainer(\n",
				"\targuments=train_arguments,\n",
				"\tmodel=model,\n",
				"\tdataset_train=packed_dataset,\n",
				")\n",
				"\n",
				"output = trainer.train(model_parameters=model_parameters, state=None)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"This code will start the training process and save model checkpoints at specified intervals.\n",
				"\n",
				"----\n",
				"\n",
				"#### 8. Save and Upload the Model\n",
				"After training, save the model and upload it to the Hugging Face Hub."
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Create a new repository or update an existing one\n",
				"api.create_repo(new_repo_id, private=True, exist_ok=True)\n",
				"\n",
				"# Save the trained model\n",
				"file_path = \"/\".join(output.checkpoint_path.split(\"/\")[:-1])\n",
				"output.state.module.save_pretrained(\n",
				"\tfile_path, output.state.params[\"params\"], float_dtype=dtype\n",
				")\n",
				"\n",
				"# Upload the model to the Hugging Face Hub\n",
				"api.upload_folder(\n",
				"\trepo_id=new_repo_id,\n",
				"\tfolder_path=file_path,\n",
				"\tignore_patterns=\"events.out.tfevents.*\",\n",
				")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"This step allows you to save and share your trained model, making it accessible for future use.\n",
				"\n",
				"----\n",
				"\n",
				"#### Conclusion\n",
				"You've now trained a causal language model using `EasyDeL`! Feel free to explore more configuration options and try different datasets to see how the model's performance varies. For further customization and detailed explanations, refer to the `EasyDeL` documentation.\n",
				"\n",
				"Happy training!"
			]
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "jax-env",
			"language": "python",
			"name": "python3"
		},
		"language_info": {
			"name": "python",
			"version": "3.10.12"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 2
}