
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
        <meta name="author" content="Erfan Zare Chavoshi">
      
      
      
        <link rel="prev" href="../generated-modules-mixtral-mixtral_configuration/">
      
      
        <link rel="next" href="../generated-modules-mosaic_mpt-modelling_mpt_flax/">
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.0, mkdocs-material-9.5.25">
    
    
      
        <title>Modelling Mixtral Flax - EasyDeL</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.6543a935.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../assets/_mkdocstrings.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#modulesmixtralmodelling_mixtral_flax" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="EasyDeL" class="md-header__button md-logo" aria-label="EasyDeL" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            EasyDeL
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Modelling Mixtral Flax
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/erfanzar/EasyDeL" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="EasyDeL" class="md-nav__button md-logo" aria-label="EasyDeL" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    EasyDeL
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/erfanzar/EasyDeL" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
    
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1" checked>
        
          
          <label class="md-nav__link" for="__nav_1" id="__nav_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    APIs
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_1_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_1">
            <span class="md-nav__icon md-icon"></span>
            APIs
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1_1" >
        
          
          <label class="md-nav__link" for="__nav_1_1" id="__nav_1_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Cli
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_1_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1_1">
            <span class="md-nav__icon md-icon"></span>
            Cli
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-cli-cli/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Cli
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1_1_2" >
        
          
          <label class="md-nav__link" for="__nav_1_1_2" id="__nav_1_1_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Train
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_1_1_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1_1_2">
            <span class="md-nav__icon md-icon"></span>
            Train
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-cli-train-cl_train_cli/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Cl Train Cli
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1_2" >
        
          
          <label class="md-nav__link" for="__nav_1_2" id="__nav_1_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Data Preprocessing
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_1_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1_2">
            <span class="md-nav__icon md-icon"></span>
            Data Preprocessing
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-data_preprocessing-data_processor/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Data Processor
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1_3" >
        
          
          <label class="md-nav__link" for="__nav_1_3" id="__nav_1_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Etils
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_1_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1_3">
            <span class="md-nav__icon md-icon"></span>
            Etils
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-etils-auto_tx/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Auto Tx
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-etils-configs/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Configs
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-etils-easystate/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Easystate
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-etils-errors/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Errors
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-etils-etils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Etils
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
    
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1_4" checked>
        
          
          <label class="md-nav__link" for="__nav_1_4" id="__nav_1_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Modules
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_1_4_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_1_4">
            <span class="md-nav__icon md-icon"></span>
            Modules
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1_4_1" >
        
          
          <label class="md-nav__link" for="__nav_1_4_1" id="__nav_1_4_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Arctic
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_1_4_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1_4_1">
            <span class="md-nav__icon md-icon"></span>
            Arctic
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-arctic-arctic_configuration/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Arctic Configuration
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-arctic-modelling_arctic_flax/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Modelling Arctic Flax
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-attention_module/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Attention Module
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-auto_easydel_model/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Auto Easydel Model
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-_blockwise_attention/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Blockwise Attention
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1_4_5" >
        
          
          <label class="md-nav__link" for="__nav_1_4_5" id="__nav_1_4_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Cohere
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_1_4_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1_4_5">
            <span class="md-nav__icon md-icon"></span>
            Cohere
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-cohere-cohere_configuration/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Cohere Configuration
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-cohere-modelling_cohere_flax/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Modelling Cohere Flax
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1_4_6" >
        
          
          <label class="md-nav__link" for="__nav_1_4_6" id="__nav_1_4_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Dbrx
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_1_4_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1_4_6">
            <span class="md-nav__icon md-icon"></span>
            Dbrx
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-dbrx-dbrx_configuration/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Dbrx Configuration
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-dbrx-modelling_dbrx_flax/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Modelling Dbrx Flax
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1_4_7" >
        
          
          <label class="md-nav__link" for="__nav_1_4_7" id="__nav_1_4_7_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Deepseek V2
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_1_4_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1_4_7">
            <span class="md-nav__icon md-icon"></span>
            Deepseek V2
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-deepseek_v2-deepseek_configuration/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Deepseek Configuration
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-deepseek_v2-modeling_deepseek_flax/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Modeling Deepseek Flax
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-easydel_modelling_utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Easydel Modelling Utils
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1_4_9" >
        
          
          <label class="md-nav__link" for="__nav_1_4_9" id="__nav_1_4_9_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Falcon
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_1_4_9_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1_4_9">
            <span class="md-nav__icon md-icon"></span>
            Falcon
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-falcon-falcon_configuration/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Falcon Configuration
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-falcon-modelling_falcon_flax/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Modelling Falcon Flax
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-flax_modelling_utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Flax Modelling Utils
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1_4_11" >
        
          
          <label class="md-nav__link" for="__nav_1_4_11" id="__nav_1_4_11_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Gemma
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_1_4_11_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1_4_11">
            <span class="md-nav__icon md-icon"></span>
            Gemma
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-gemma-gemma_configuration/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Gemma Configuration
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-gemma-modelling_gemma_flax/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Modelling Gemma Flax
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1_4_12" >
        
          
          <label class="md-nav__link" for="__nav_1_4_12" id="__nav_1_4_12_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Gpt J
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_1_4_12_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1_4_12">
            <span class="md-nav__icon md-icon"></span>
            Gpt J
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-gpt_j-gpt_j_configuration/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Gpt J Configuration
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-gpt_j-modelling_gpt_j_flax/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Modelling Gpt J Flax
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1_4_13" >
        
          
          <label class="md-nav__link" for="__nav_1_4_13" id="__nav_1_4_13_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Gpt Neo X
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_1_4_13_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1_4_13">
            <span class="md-nav__icon md-icon"></span>
            Gpt Neo X
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-gpt_neo_x-gpt_neo_x_configuration/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Gpt Neo X Configuration
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-gpt_neo_x-modelling_gpt_neo_x_flax/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Modelling Gpt Neo X Flax
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1_4_14" >
        
          
          <label class="md-nav__link" for="__nav_1_4_14" id="__nav_1_4_14_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Gpt2
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_1_4_14_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1_4_14">
            <span class="md-nav__icon md-icon"></span>
            Gpt2
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-gpt2-gpt2_configuration/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Gpt2 Configuration
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-gpt2-modelling_gpt2_flax/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Modelling Gpt2 Flax
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1_4_15" >
        
          
          <label class="md-nav__link" for="__nav_1_4_15" id="__nav_1_4_15_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Grok 1
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_1_4_15_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1_4_15">
            <span class="md-nav__icon md-icon"></span>
            Grok 1
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-grok_1-grok_1_configuration/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Grok 1 Configuration
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-grok_1-modelling_grok_1_flax/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Modelling Grok 1 Flax
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1_4_16" >
        
          
          <label class="md-nav__link" for="__nav_1_4_16" id="__nav_1_4_16_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Jetmoe
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_1_4_16_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1_4_16">
            <span class="md-nav__icon md-icon"></span>
            Jetmoe
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-jetmoe-jetmoe_configuration/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Jetmoe Configuration
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-jetmoe-modelling_jetmoe_flax/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Modelling Jetmoe Flax
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1_4_17" >
        
          
          <label class="md-nav__link" for="__nav_1_4_17" id="__nav_1_4_17_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Llama
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_1_4_17_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1_4_17">
            <span class="md-nav__icon md-icon"></span>
            Llama
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-llama-llama_configuration/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Llama Configuration
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-llama-modelling_llama_flax/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Modelling Llama Flax
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-llama-modelling_vision_llama_flax/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Modelling Vision Llama Flax
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-llama-vision_llama_configuration/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Vision Llama Configuration
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1_4_18" >
        
          
          <label class="md-nav__link" for="__nav_1_4_18" id="__nav_1_4_18_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Lucid Transformer
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_1_4_18_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1_4_18">
            <span class="md-nav__icon md-icon"></span>
            Lucid Transformer
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-lucid_transformer-lt_configuration/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Lt Configuration
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-lucid_transformer-modelling_lt_flax/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Modelling Lt Flax
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1_4_19" >
        
          
          <label class="md-nav__link" for="__nav_1_4_19" id="__nav_1_4_19_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Mamba
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_1_4_19_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1_4_19">
            <span class="md-nav__icon md-icon"></span>
            Mamba
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-mamba-mamba_configuration/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Mamba Configuration
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-mamba-modelling_mamba_flax/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Modelling Mamba Flax
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1_4_20" >
        
          
          <label class="md-nav__link" for="__nav_1_4_20" id="__nav_1_4_20_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Mistral
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_1_4_20_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1_4_20">
            <span class="md-nav__icon md-icon"></span>
            Mistral
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-mistral-mistral_configuration/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Mistral Configuration
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-mistral-modelling_mistral_flax/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Modelling Mistral Flax
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-mistral-modelling_vision_mistral_flax/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Modelling Vision Mistral Flax
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-mistral-vision_mistral_configuration/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Vision Mistral Configuration
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
    
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1_4_21" checked>
        
          
          <label class="md-nav__link" for="__nav_1_4_21" id="__nav_1_4_21_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Mixtral
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_1_4_21_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_1_4_21">
            <span class="md-nav__icon md-icon"></span>
            Mixtral
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-mixtral-mixtral_configuration/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Mixtral Configuration
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Modelling Mixtral Flax
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Modelling Mixtral Flax
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#src.python.easydel.modules.mixtral.modelling_mixtral_flax" class="md-nav__link">
    <span class="md-ellipsis">
      modelling_mixtral_flax
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.python.easydel.modules.mixtral.modelling_mixtral_flax.FlaxMixtralAttention" class="md-nav__link">
    <span class="md-ellipsis">
      FlaxMixtralAttention
    </span>
  </a>
  
    <nav class="md-nav" aria-label="FlaxMixtralAttention">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#src.python.easydel.modules.mixtral.modelling_mixtral_flax.FlaxMixtralAttention.__call__" class="md-nav__link">
    <span class="md-ellipsis">
      __call__
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.python.easydel.modules.mixtral.modelling_mixtral_flax.FlaxMixtralDecoderLayer" class="md-nav__link">
    <span class="md-ellipsis">
      FlaxMixtralDecoderLayer
    </span>
  </a>
  
    <nav class="md-nav" aria-label="FlaxMixtralDecoderLayer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#src.python.easydel.modules.mixtral.modelling_mixtral_flax.FlaxMixtralDecoderLayer.__call__" class="md-nav__link">
    <span class="md-ellipsis">
      __call__
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.python.easydel.modules.mixtral.modelling_mixtral_flax.FlaxMixtralDecoderLayerCollection" class="md-nav__link">
    <span class="md-ellipsis">
      FlaxMixtralDecoderLayerCollection
    </span>
  </a>
  
    <nav class="md-nav" aria-label="FlaxMixtralDecoderLayerCollection">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#src.python.easydel.modules.mixtral.modelling_mixtral_flax.FlaxMixtralDecoderLayerCollection.__call__" class="md-nav__link">
    <span class="md-ellipsis">
      __call__
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.python.easydel.modules.mixtral.modelling_mixtral_flax.FlaxMixtralForCausalLM" class="md-nav__link">
    <span class="md-ellipsis">
      FlaxMixtralForCausalLM
    </span>
  </a>
  
    <nav class="md-nav" aria-label="FlaxMixtralForCausalLM">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#src.python.easydel.modules.mixtral.modelling_mixtral_flax.FlaxMixtralForCausalLM.prepare_inputs_for_generation" class="md-nav__link">
    <span class="md-ellipsis">
      prepare_inputs_for_generation
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.python.easydel.modules.mixtral.modelling_mixtral_flax.FlaxMixtralSparseMoeBlock" class="md-nav__link">
    <span class="md-ellipsis">
      FlaxMixtralSparseMoeBlock
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.python.easydel.modules.mixtral.modelling_mixtral_flax.MixtralPreTrainedModel" class="md-nav__link">
    <span class="md-ellipsis">
      MixtralPreTrainedModel
    </span>
  </a>
  
    <nav class="md-nav" aria-label="MixtralPreTrainedModel">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#src.python.easydel.modules.mixtral.modelling_mixtral_flax.MixtralPreTrainedModel.__call__" class="md-nav__link">
    <span class="md-ellipsis">
      __call__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#src.python.easydel.modules.mixtral.modelling_mixtral_flax.MixtralPreTrainedModel.init_weights" class="md-nav__link">
    <span class="md-ellipsis">
      init_weights
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1_4_22" >
        
          
          <label class="md-nav__link" for="__nav_1_4_22" id="__nav_1_4_22_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Mosaic Mpt
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_1_4_22_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1_4_22">
            <span class="md-nav__icon md-icon"></span>
            Mosaic Mpt
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-mosaic_mpt-modelling_mpt_flax/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Modelling Mpt Flax
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-mosaic_mpt-mosaic_configuration/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Mosaic Configuration
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1_4_23" >
        
          
          <label class="md-nav__link" for="__nav_1_4_23" id="__nav_1_4_23_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Olmo
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_1_4_23_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1_4_23">
            <span class="md-nav__icon md-icon"></span>
            Olmo
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-olmo-modelling_olmo_flax/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Modelling Olmo Flax
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-olmo-olmo_configuration/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Olmo Configuration
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1_4_24" >
        
          
          <label class="md-nav__link" for="__nav_1_4_24" id="__nav_1_4_24_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Openelm
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_1_4_24_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1_4_24">
            <span class="md-nav__icon md-icon"></span>
            Openelm
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-openelm-modelling_openelm_flax/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Modelling Openelm Flax
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-openelm-openelm_configuration/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Openelm Configuration
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1_4_25" >
        
          
          <label class="md-nav__link" for="__nav_1_4_25" id="__nav_1_4_25_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Opt
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_1_4_25_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1_4_25">
            <span class="md-nav__icon md-icon"></span>
            Opt
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-opt-modelling_opt_flax/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Modelling Opt Flax
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-opt-opt_configuration/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Opt Configuration
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1_4_26" >
        
          
          <label class="md-nav__link" for="__nav_1_4_26" id="__nav_1_4_26_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Palm
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_1_4_26_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1_4_26">
            <span class="md-nav__icon md-icon"></span>
            Palm
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-palm-modelling_palm_flax/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Modelling Palm Flax
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-palm-palm_configuration/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Palm Configuration
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1_4_27" >
        
          
          <label class="md-nav__link" for="__nav_1_4_27" id="__nav_1_4_27_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Phi
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_1_4_27_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1_4_27">
            <span class="md-nav__icon md-icon"></span>
            Phi
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-phi-modelling_phi_flax/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Modelling Phi Flax
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-phi-phi_configuration/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Phi Configuration
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1_4_28" >
        
          
          <label class="md-nav__link" for="__nav_1_4_28" id="__nav_1_4_28_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Phi3
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_1_4_28_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1_4_28">
            <span class="md-nav__icon md-icon"></span>
            Phi3
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-phi3-modelling_phi3_flax/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Modelling Phi3 Flax
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-phi3-phi3_configuration/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Phi3 Configuration
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1_4_29" >
        
          
          <label class="md-nav__link" for="__nav_1_4_29" id="__nav_1_4_29_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Qwen1
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_1_4_29_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1_4_29">
            <span class="md-nav__icon md-icon"></span>
            Qwen1
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-qwen1-modelling_qwen1_flax/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Modelling Qwen1 Flax
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-qwen1-qwen1_configuration/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Qwen1 Configuration
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1_4_30" >
        
          
          <label class="md-nav__link" for="__nav_1_4_30" id="__nav_1_4_30_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Qwen2
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_1_4_30_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1_4_30">
            <span class="md-nav__icon md-icon"></span>
            Qwen2
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-qwen2-modelling_qwen_flax/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Modelling Qwen Flax
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-qwen2-qwen_configuration/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Qwen Configuration
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1_4_31" >
        
          
          <label class="md-nav__link" for="__nav_1_4_31" id="__nav_1_4_31_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Qwen2 Moe
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_1_4_31_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1_4_31">
            <span class="md-nav__icon md-icon"></span>
            Qwen2 Moe
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-qwen2_moe-configuration_qwen2_moe/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Configuration Qwen2 Moe
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-qwen2_moe-modeling_qwen2_moe_flax/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Modeling Qwen2 Moe Flax
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-_ring_attention/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Ring Attention
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1_4_33" >
        
          
          <label class="md-nav__link" for="__nav_1_4_33" id="__nav_1_4_33_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Roberta
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_1_4_33_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1_4_33">
            <span class="md-nav__icon md-icon"></span>
            Roberta
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-roberta-modelling_roberta_flax/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Modelling Roberta Flax
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-roberta-roberta_configuration/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Roberta Configuration
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1_4_34" >
        
          
          <label class="md-nav__link" for="__nav_1_4_34" id="__nav_1_4_34_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Rwkv
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_1_4_34_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1_4_34">
            <span class="md-nav__icon md-icon"></span>
            Rwkv
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-rwkv-modelling_rwkv_flax/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Modelling Rwkv Flax
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-rwkv-rwkv_configuration/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Rwkv Configuration
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1_4_35" >
        
          
          <label class="md-nav__link" for="__nav_1_4_35" id="__nav_1_4_35_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Stablelm
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_1_4_35_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1_4_35">
            <span class="md-nav__icon md-icon"></span>
            Stablelm
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-stablelm-modelling_stablelm_flax/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Modelling Stablelm Flax
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-stablelm-stablelm_configuration/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Stablelm Configuration
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1_4_36" >
        
          
          <label class="md-nav__link" for="__nav_1_4_36" id="__nav_1_4_36_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    T5
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_1_4_36_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1_4_36">
            <span class="md-nav__icon md-icon"></span>
            T5
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-t5-modelling_t5_flax/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Modelling T5 Flax
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-t5-t5_configuration/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    T5 Configuration
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-_vanilla_attention/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Vanilla Attention
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1_4_38" >
        
          
          <label class="md-nav__link" for="__nav_1_4_38" id="__nav_1_4_38_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Whisper
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_1_4_38_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1_4_38">
            <span class="md-nav__icon md-icon"></span>
            Whisper
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-whisper-modelling_whisper_flax/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Modelling Whisper Flax
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-modules-whisper-whisper_configuration/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Whisper Configuration
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1_5" >
        
          
          <label class="md-nav__link" for="__nav_1_5" id="__nav_1_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Reinforcement Learning
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_1_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1_5">
            <span class="md-nav__icon md-icon"></span>
            Reinforcement Learning
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-reinforcement_learning-core/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Core
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1_5_2" >
        
          
          <label class="md-nav__link" for="__nav_1_5_2" id="__nav_1_5_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Models
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_1_5_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1_5_2">
            <span class="md-nav__icon md-icon"></span>
            Models
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-reinforcement_learning-models-modelling_casual_language_rl/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Modelling Casual Language Rl
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1_5_3" >
        
          
          <label class="md-nav__link" for="__nav_1_5_3" id="__nav_1_5_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Trainer
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_1_5_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1_5_3">
            <span class="md-nav__icon md-icon"></span>
            Trainer
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-reinforcement_learning-trainer-partitioner_config/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Partitioner Config
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-reinforcement_learning-trainer-ppo_config/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Ppo Config
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-reinforcement_learning-trainer-ppo_trainer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Ppo Trainer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-reinforcement_learning-trainer-training_configs/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Training Configs
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-reinforcement_learning-trainer-utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Utils
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1_5_4" >
        
          
          <label class="md-nav__link" for="__nav_1_5_4" id="__nav_1_5_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Utils
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_1_5_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1_5_4">
            <span class="md-nav__icon md-icon"></span>
            Utils
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-reinforcement_learning-utils-collectors/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Collectors
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1_6" >
        
          
          <label class="md-nav__link" for="__nav_1_6" id="__nav_1_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Serve
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_1_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1_6">
            <span class="md-nav__icon md-icon"></span>
            Serve
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-serve-client/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Client
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-serve-configuration/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Configuration
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-serve-serve/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Serve
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1_7" >
        
          
          <label class="md-nav__link" for="__nav_1_7" id="__nav_1_7_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Smi
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_1_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1_7">
            <span class="md-nav__icon md-icon"></span>
            Smi
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-smi-smi/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Smi
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1_8" >
        
          
          <label class="md-nav__link" for="__nav_1_8" id="__nav_1_8_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Trainer
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_1_8_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1_8">
            <span class="md-nav__icon md-icon"></span>
            Trainer
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-trainer-base_trainer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Base Trainer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1_8_2" >
        
          
          <label class="md-nav__link" for="__nav_1_8_2" id="__nav_1_8_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Causal Language Model Trainer
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_1_8_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1_8_2">
            <span class="md-nav__icon md-icon"></span>
            Causal Language Model Trainer
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-trainer-causal_language_model_trainer-causal_language_model_trainer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Causal Language Model Trainer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-trainer-causal_language_model_trainer-fwd_bwd_functions/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Fwd Bwd Functions
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-trainer-causal_language_model_trainer-modeling_output/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Modeling Output
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1_8_3" >
        
          
          <label class="md-nav__link" for="__nav_1_8_3" id="__nav_1_8_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Dpo
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_1_8_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1_8_3">
            <span class="md-nav__icon md-icon"></span>
            Dpo
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-trainer-dpo-dpo_trainer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Dpo Trainer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-trainer-dpo-fwd_bwd_functions/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Fwd Bwd Functions
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-trainer-dpo-modelling_output/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Modelling Output
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-trainer-dpo-utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Utils
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1_8_4" >
        
          
          <label class="md-nav__link" for="__nav_1_8_4" id="__nav_1_8_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Orpo
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_1_8_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1_8_4">
            <span class="md-nav__icon md-icon"></span>
            Orpo
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-trainer-orpo-fwd_bwd_functions/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Fwd Bwd Functions
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-trainer-orpo-modelling_output/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Modelling Output
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-trainer-orpo-orpo_trainer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Orpo Trainer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-trainer-orpo-utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Utils
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1_8_5" >
        
          
          <label class="md-nav__link" for="__nav_1_8_5" id="__nav_1_8_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Sft
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_1_8_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1_8_5">
            <span class="md-nav__icon md-icon"></span>
            Sft
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-trainer-sft-stf_trainer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Stf Trainer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-trainer-sft-utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Utils
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-trainer-training_configurations/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Training Configurations
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-trainer-utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Utils
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1_8_8" >
        
          
          <label class="md-nav__link" for="__nav_1_8_8" id="__nav_1_8_8_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Vision Causal Language Model Trainer
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_1_8_8_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1_8_8">
            <span class="md-nav__icon md-icon"></span>
            Vision Causal Language Model Trainer
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-trainer-vision_causal_language_model_trainer-fwd_bwd_functions/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Fwd Bwd Functions
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-trainer-vision_causal_language_model_trainer-modelling_output/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Modelling Output
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-trainer-vision_causal_language_model_trainer-vision_causal_language_model_trainer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Vision Causal Language Model Trainer
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1_9" >
        
          
          <label class="md-nav__link" for="__nav_1_9" id="__nav_1_9_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Transform
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_1_9_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1_9">
            <span class="md-nav__icon md-icon"></span>
            Transform
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-transform-easydel_transform/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Easydel Transform
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-transform-utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Utils
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1_10" >
        
          
          <label class="md-nav__link" for="__nav_1_10" id="__nav_1_10_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Utils
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_1_10_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1_10">
            <span class="md-nav__icon md-icon"></span>
            Utils
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-utils-checker/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Checker
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-utils-prompters/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Prompters
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-utils-tensor_utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Tensor Utils
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generated-utils-utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Utils
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../AvailableModels.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Available models
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../CONTRIBUTING/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Contributing
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Examples
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Examples
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../DataProcessing/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    DataProcessing
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../AttentionModuleExample.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Easy Attention
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../EasyStateExample/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    EasyState
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Falcon.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Falcon Models
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../FineTuningExample/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Fine Tuning Example
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Llama.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Llama Models
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Llama2.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Llama2 Models
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../LoRA-TransferLearningExample/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LoRA and Transfer Learning
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Mistral.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Mistral Models
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Parameter-Quantization/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Model Parameter Quantization
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../MosaicMPT.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    MosaicMPT Models
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../Install/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Install
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#src.python.easydel.modules.mixtral.modelling_mixtral_flax" class="md-nav__link">
    <span class="md-ellipsis">
      modelling_mixtral_flax
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.python.easydel.modules.mixtral.modelling_mixtral_flax.FlaxMixtralAttention" class="md-nav__link">
    <span class="md-ellipsis">
      FlaxMixtralAttention
    </span>
  </a>
  
    <nav class="md-nav" aria-label="FlaxMixtralAttention">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#src.python.easydel.modules.mixtral.modelling_mixtral_flax.FlaxMixtralAttention.__call__" class="md-nav__link">
    <span class="md-ellipsis">
      __call__
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.python.easydel.modules.mixtral.modelling_mixtral_flax.FlaxMixtralDecoderLayer" class="md-nav__link">
    <span class="md-ellipsis">
      FlaxMixtralDecoderLayer
    </span>
  </a>
  
    <nav class="md-nav" aria-label="FlaxMixtralDecoderLayer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#src.python.easydel.modules.mixtral.modelling_mixtral_flax.FlaxMixtralDecoderLayer.__call__" class="md-nav__link">
    <span class="md-ellipsis">
      __call__
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.python.easydel.modules.mixtral.modelling_mixtral_flax.FlaxMixtralDecoderLayerCollection" class="md-nav__link">
    <span class="md-ellipsis">
      FlaxMixtralDecoderLayerCollection
    </span>
  </a>
  
    <nav class="md-nav" aria-label="FlaxMixtralDecoderLayerCollection">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#src.python.easydel.modules.mixtral.modelling_mixtral_flax.FlaxMixtralDecoderLayerCollection.__call__" class="md-nav__link">
    <span class="md-ellipsis">
      __call__
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.python.easydel.modules.mixtral.modelling_mixtral_flax.FlaxMixtralForCausalLM" class="md-nav__link">
    <span class="md-ellipsis">
      FlaxMixtralForCausalLM
    </span>
  </a>
  
    <nav class="md-nav" aria-label="FlaxMixtralForCausalLM">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#src.python.easydel.modules.mixtral.modelling_mixtral_flax.FlaxMixtralForCausalLM.prepare_inputs_for_generation" class="md-nav__link">
    <span class="md-ellipsis">
      prepare_inputs_for_generation
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.python.easydel.modules.mixtral.modelling_mixtral_flax.FlaxMixtralSparseMoeBlock" class="md-nav__link">
    <span class="md-ellipsis">
      FlaxMixtralSparseMoeBlock
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.python.easydel.modules.mixtral.modelling_mixtral_flax.MixtralPreTrainedModel" class="md-nav__link">
    <span class="md-ellipsis">
      MixtralPreTrainedModel
    </span>
  </a>
  
    <nav class="md-nav" aria-label="MixtralPreTrainedModel">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#src.python.easydel.modules.mixtral.modelling_mixtral_flax.MixtralPreTrainedModel.__call__" class="md-nav__link">
    <span class="md-ellipsis">
      __call__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#src.python.easydel.modules.mixtral.modelling_mixtral_flax.MixtralPreTrainedModel.init_weights" class="md-nav__link">
    <span class="md-ellipsis">
      init_weights
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
  


<h1 id="modulesmixtralmodelling_mixtral_flax">modules.mixtral.modelling_mixtral_flax</h1>


<div class="doc doc-object doc-module">



<a id="src.python.easydel.modules.mixtral.modelling_mixtral_flax"></a>
    <div class="doc doc-contents first">



  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h2 id="src.python.easydel.modules.mixtral.modelling_mixtral_flax.FlaxMixtralAttention" class="doc doc-heading">
            <code>FlaxMixtralAttention</code>


</h2>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="src.python.easydel.modules.flax_modelling_utils.BaseJAXAttentionModule" href="../generated-modules-flax_modelling_utils/#src.python.easydel.modules.flax_modelling_utils.BaseJAXAttentionModule">BaseJAXAttentionModule</a></code></p>


              <details class="quote">
                <summary>Source code in <code>src/python/easydel/modules/mixtral/modelling_mixtral_flax.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">FlaxMixtralAttention</span><span class="p">(</span><span class="n">BaseJAXAttentionModule</span><span class="p">):</span>
    <span class="n">config</span><span class="p">:</span> <span class="n">MixtralConfig</span>
    <span class="n">layer_index</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">dtype</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">bfloat16</span>
    <span class="n">param_dtype</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">bfloat16</span>
    <span class="n">precision</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">Precision</span><span class="p">]]</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">Precision</span><span class="p">(</span><span class="s2">&quot;fastest&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">setup</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">num_attention_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_key_value_heads</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">num_key_value_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_key_value_groups</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_key_value_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_position_embeddings</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">max_position_embeddings</span>

        <span class="n">dense</span> <span class="o">=</span> <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span>
            <span class="n">Linear</span><span class="p">,</span>
            <span class="n">use_bias</span><span class="o">=</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="s2">&quot;attention_bias&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
            <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">param_dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">param_dtype</span><span class="p">,</span>
            <span class="n">precision</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">precision</span><span class="p">,</span>
            <span class="n">kernel_init</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">initializers</span><span class="o">.</span><span class="n">normal</span><span class="p">(),</span>
            <span class="o">**</span><span class="n">get_dot_general_by_bits</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">bits</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">easy_method</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span> <span class="o">=</span> <span class="n">dense</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span> <span class="o">=</span> <span class="n">dense</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_key_value_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span> <span class="o">=</span> <span class="n">dense</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_key_value_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">o_proj</span> <span class="o">=</span> <span class="n">dense</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rotary</span> <span class="o">=</span> <span class="n">FlaxMixtralRotaryEmbedding</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention_performer</span> <span class="o">=</span> <span class="n">AttentionModule</span><span class="p">(</span>
            <span class="n">use_sharding_constraint</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_sharding_constraint</span><span class="p">,</span>
            <span class="n">block_k_major</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">block_k_major</span><span class="p">,</span>
            <span class="n">block_b</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">block_b</span><span class="p">,</span>
            <span class="n">block_q</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">block_q</span><span class="p">,</span>
            <span class="n">block_k</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">block_k</span><span class="p">,</span>
            <span class="n">block_q_major_dkv</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">block_q_major_dkv</span><span class="p">,</span>
            <span class="n">block_k_major_dkv</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">block_k_major_dkv</span><span class="p">,</span>
            <span class="n">block_k_major_dq</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">block_k_major_dq</span><span class="p">,</span>
            <span class="n">block_k_dkv</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">block_k_dkv</span><span class="p">,</span>
            <span class="n">block_q_dkv</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">block_q_dkv</span><span class="p">,</span>
            <span class="n">block_q_dq</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">block_q_dq</span><span class="p">,</span>
            <span class="n">block_k_dq</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">block_k_dq</span><span class="p">,</span>
            <span class="n">num_attention_heads</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="p">,</span>
            <span class="n">attention_dropout</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">attention_dropout</span><span class="p">,</span>
            <span class="n">head_dims</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">,</span>
            <span class="n">attention_partition_spec</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">attention_partition_spec</span><span class="p">,</span>
            <span class="n">shard_attention_computation</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">shard_attention_computation</span><span class="p">,</span>
            <span class="n">precision</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">precision</span><span class="p">,</span>
            <span class="n">force_float32_tpu</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">attn_mechanism</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">attn_mechanism</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">bias_partition_spec</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">bias_partition_spec</span><span class="p">,</span>
            <span class="n">key_partition_spec</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">key_partition_spec</span><span class="p">,</span>
            <span class="n">query_partition_spec</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">query_partition_spec</span><span class="p">,</span>
            <span class="n">generation_query_partition_spec</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">generation_query_partition_spec</span><span class="p">,</span>
            <span class="n">generation_bias_partition_spec</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">generation_bias_partition_spec</span><span class="p">,</span>
            <span class="n">generation_attention_partition_spec</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">generation_attention_partition_spec</span><span class="p">,</span>
            <span class="n">value_partition_spec</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">value_partition_spec</span><span class="p">,</span>
            <span class="n">scan_ring_attention</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">scan_ring_attention</span><span class="p">,</span>
            <span class="n">mesh</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">jax_mesh</span><span class="p">(),</span>
            <span class="n">sm_scale</span><span class="o">=</span><span class="mi">1</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">),</span>
            <span class="n">axis_name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">attention_axis_name</span>
        <span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_transpose_sequence_head</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)),</span> <span class="n">jnp</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)),</span> <span class="n">jnp</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">apply_rotary</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">freq_cis</span><span class="p">,</span> <span class="n">position_ids</span><span class="p">):</span>
        <span class="n">query</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span>
                              <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span>
                          <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_key_value_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span>
                              <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_key_value_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>

        <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_transpose_sequence_head</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
        <span class="n">query</span><span class="p">,</span> <span class="n">key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rotary</span><span class="p">(</span>
            <span class="n">position_ids</span><span class="o">=</span><span class="n">position_ids</span><span class="p">,</span> <span class="n">query</span><span class="o">=</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">key</span><span class="p">,</span> <span class="n">freq_cis</span><span class="o">=</span><span class="n">freq_cis</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="n">repeat_kv_bnsh</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_key_value_groups</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">repeat_kv_bnsh</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_key_value_groups</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_transpose_sequence_head</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_merge_heads</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,))</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">hidden_states</span><span class="p">:</span> <span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span>
            <span class="n">freq_cis</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span> <span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">],</span>
            <span class="n">attention_mask</span><span class="p">:</span> <span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span>
            <span class="n">causal_mask</span><span class="p">:</span> <span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span>
            <span class="n">position_ids</span><span class="p">:</span> <span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span>
            <span class="n">segment_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">deterministic</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
            <span class="n">init_cache</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
            <span class="n">output_attentions</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;The __call__ function is the main function of a JAX module.</span>
<span class="sd">        It defines how the module behaves when called as a function, and it&#39;s what you&#39;ll use to call your model in practice.</span>
<span class="sd">        The __call__ method takes an input tensor (x) and returns an output tensor (y).</span>
<span class="sd">        In this case, we&#39;re defining our model to be a simple linear layer with no activation: y = x @ w + b.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: Refer to the object itself</span>
<span class="sd">            hidden_states: chex.Array: Pass in the hidden state of the</span>
<span class="sd">                model</span>
<span class="sd">            freq_cis: Tuple[chex.Array, chex.Array],: Create the</span>
<span class="sd">                apply_rotary variable</span>
<span class="sd">            attention_mask: chex.Array: Mask the attention weights</span>
<span class="sd">            causal_mask: chex.Array: Mask the attention weights</span>
<span class="sd">            position_ids: chex.Array: Specify the position of each token</span>
<span class="sd">                in a sequence</span>
<span class="sd">            deterministic: bool: Determine whether to use dropout or not</span>
<span class="sd">            init_cache: bool: Initialize the cache</span>
<span class="sd">            output_attentions: bool: Determine whether to return the</span>
<span class="sd">                attention weights</span>

<span class="sd">        Returns:</span>
<span class="sd">            A tuple of (out, attn_output)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">query_states</span><span class="p">,</span> <span class="n">key_states</span><span class="p">,</span> <span class="n">value_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span><span class="p">(</span>
            <span class="n">hidden_states</span><span class="p">)</span>

        <span class="n">query_states</span> <span class="o">=</span> <span class="n">query_states</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
            <span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>
        <span class="n">key_states</span> <span class="o">=</span> <span class="n">key_states</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
            <span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_key_value_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>
        <span class="n">value_states</span> <span class="o">=</span> <span class="n">value_states</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
            <span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_key_value_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>

        <span class="n">query_states</span><span class="p">,</span> <span class="n">key_states</span><span class="p">,</span> <span class="n">value_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_rotary</span><span class="p">(</span>
            <span class="n">query</span><span class="o">=</span><span class="n">query_states</span><span class="p">,</span>
            <span class="n">key</span><span class="o">=</span><span class="n">key_states</span><span class="p">,</span>
            <span class="n">value</span><span class="o">=</span><span class="n">value_states</span><span class="p">,</span>
            <span class="n">position_ids</span><span class="o">=</span><span class="n">position_ids</span><span class="p">,</span>
            <span class="n">freq_cis</span><span class="o">=</span><span class="n">freq_cis</span><span class="p">,</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
            <span class="n">sequence_length</span><span class="o">=</span><span class="n">sequence_length</span>
        <span class="p">)</span>

        <span class="n">assert_msg</span> <span class="o">=</span> <span class="p">(</span>
            <span class="s2">&quot;num_attention_heads repeat wont work likely</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="sa">f</span><span class="s2">&quot;INFO :</span><span class="se">\n\t</span><span class="s2">repeat_kv_bnsh Used with num_key_value_groups = </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">num_key_value_groups</span><span class="si">}</span><span class="se">\n\t</span><span class="s2">&quot;</span>
            <span class="sa">f</span><span class="s2">&quot;NH : </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="si">}</span><span class="s2"> KVH : </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

        <span class="k">assert</span> <span class="n">query_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="p">,</span> <span class="n">assert_msg</span>
        <span class="k">assert</span> <span class="n">key_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="p">,</span> <span class="n">assert_msg</span>
        <span class="k">assert</span> <span class="n">value_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="p">,</span> <span class="n">assert_msg</span>

        <span class="n">query_length</span><span class="p">,</span> <span class="n">key_length</span> <span class="o">=</span> <span class="n">query_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">key_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_variable</span><span class="p">(</span><span class="s2">&quot;cache&quot;</span><span class="p">,</span> <span class="s2">&quot;cached_key&quot;</span><span class="p">):</span>
            <span class="n">mask_shift</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">variables</span><span class="p">[</span><span class="s2">&quot;cache&quot;</span><span class="p">][</span><span class="s2">&quot;cache_index&quot;</span><span class="p">]</span>
            <span class="n">max_decoder_length</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">variables</span><span class="p">[</span><span class="s2">&quot;cache&quot;</span><span class="p">][</span><span class="s2">&quot;cached_key&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">causal_mask</span> <span class="o">=</span> <span class="n">lax</span><span class="o">.</span><span class="n">dynamic_slice</span><span class="p">(</span>
                <span class="n">causal_mask</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">mask_shift</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
                                                     <span class="n">query_length</span><span class="p">,</span> <span class="n">max_decoder_length</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">causal_mask</span> <span class="o">=</span> <span class="n">causal_mask</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:</span><span class="n">query_length</span><span class="p">,</span> <span class="p">:</span><span class="n">key_length</span><span class="p">]</span>

        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">causal_mask</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span>
            <span class="n">causal_mask</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,)</span> <span class="o">+</span> <span class="n">causal_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span>
            <span class="n">attention_mask</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)),</span> <span class="n">causal_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">combine_masks</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">causal_mask</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span>

        <span class="n">dropout_rng</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">deterministic</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">attention_dropout</span> <span class="o">&gt;</span> <span class="mf">0.0</span><span class="p">:</span>
            <span class="n">dropout_rng</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">make_rng</span><span class="p">(</span><span class="s2">&quot;dropout&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_variable</span><span class="p">(</span><span class="s2">&quot;cache&quot;</span><span class="p">,</span> <span class="s2">&quot;cached_key&quot;</span><span class="p">)</span> <span class="ow">or</span> <span class="n">init_cache</span><span class="p">:</span>
            <span class="n">key_states</span><span class="p">,</span> <span class="n">value_states</span><span class="p">,</span> <span class="n">attention_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_concatenate_to_cache</span><span class="p">(</span>
                <span class="n">key_states</span><span class="p">,</span>
                <span class="n">value_states</span><span class="p">,</span>
                <span class="n">query_states</span><span class="p">,</span>
                <span class="n">attention_mask</span>
            <span class="p">)</span>
        <span class="c1"># if self.config.use_sharding_constraint:</span>
        <span class="c1">#     query_states = with_sharding_constraint(</span>
        <span class="c1">#         query_states, PartitionSpec((&quot;dp&quot;, &quot;fsdp&quot;), &quot;sp&quot; if query_states.shape[1] != 1 else None, &quot;tp&quot;, None)</span>
        <span class="c1">#     )</span>
        <span class="c1">#     key_states = with_sharding_constraint(</span>
        <span class="c1">#         key_states, PartitionSpec((&quot;dp&quot;, &quot;fsdp&quot;), &quot;sp&quot;, &quot;tp&quot;, None)</span>
        <span class="c1">#     )</span>
        <span class="c1">#     value_states = with_sharding_constraint(</span>
        <span class="c1">#         value_states, PartitionSpec((&quot;dp&quot;, &quot;fsdp&quot;), &quot;sp&quot;, &quot;tp&quot;, None)</span>
        <span class="c1">#     )</span>
        <span class="n">attention_bias</span> <span class="o">=</span> <span class="n">lax</span><span class="o">.</span><span class="n">select</span><span class="p">(</span>
            <span class="n">attention_mask</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span>
            <span class="n">jnp</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="n">attention_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span>
            <span class="n">jnp</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="n">attention_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">min</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span>
        <span class="p">)</span>

        <span class="n">query_length</span><span class="p">,</span> <span class="n">key_length</span> <span class="o">=</span> <span class="n">query_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">key_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="n">attentions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_performer</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span>
            <span class="n">query_states</span><span class="o">=</span><span class="n">query_states</span><span class="p">,</span>
            <span class="n">key_states</span><span class="o">=</span><span class="n">key_states</span><span class="p">,</span>
            <span class="n">value_states</span><span class="o">=</span><span class="n">value_states</span><span class="p">,</span>
            <span class="n">bias</span><span class="o">=</span><span class="n">attention_bias</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
            <span class="n">causal</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">dropout_rng</span><span class="o">=</span><span class="n">dropout_rng</span><span class="p">,</span>
            <span class="n">deterministic</span><span class="o">=</span><span class="n">deterministic</span><span class="p">,</span>
            <span class="n">query_sequence_length</span><span class="o">=</span><span class="n">query_length</span><span class="p">,</span>
            <span class="n">key_value_sequence_length</span><span class="o">=</span><span class="n">key_length</span><span class="p">,</span>
            <span class="n">uses_cache</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">has_variable</span><span class="p">(</span><span class="s2">&quot;cache&quot;</span><span class="p">,</span> <span class="s2">&quot;cached_key&quot;</span><span class="p">)</span> <span class="ow">or</span> <span class="n">init_cache</span><span class="p">,</span>
            <span class="n">segment_ids</span><span class="o">=</span><span class="n">segment_ids</span><span class="p">,</span>
            <span class="n">causal_mask</span><span class="o">=</span><span class="n">causal_mask</span>
        <span class="p">)</span>

        <span class="n">attn_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_merge_heads</span><span class="p">(</span><span class="n">attentions</span><span class="o">.</span><span class="n">attention_outputs</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">shard_attention_computation</span><span class="p">:</span>
            <span class="n">attn_output</span> <span class="o">=</span> <span class="n">with_sharding_constraint</span><span class="p">(</span>
                <span class="n">attn_output</span><span class="p">,</span> <span class="n">PartitionSpec</span><span class="p">(</span>
                    <span class="p">(</span><span class="s2">&quot;dp&quot;</span><span class="p">,</span> <span class="s2">&quot;fsdp&quot;</span><span class="p">),</span>
                    <span class="s2">&quot;sp&quot;</span> <span class="k">if</span> <span class="n">attn_output</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">1</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
                    <span class="s2">&quot;tp&quot;</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="n">attn_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">o_proj</span><span class="p">(</span><span class="n">attn_output</span><span class="p">)</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">attn_output</span><span class="p">,</span> <span class="n">attentions</span><span class="o">.</span><span class="n">attention_weights</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">outputs</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="src.python.easydel.modules.mixtral.modelling_mixtral_flax.FlaxMixtralAttention.__call__" class="doc doc-heading">
            <code class="highlight language-python"><span class="fm">__call__</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">freq_cis</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">causal_mask</span><span class="p">,</span> <span class="n">position_ids</span><span class="p">,</span> <span class="n">segment_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">deterministic</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">init_cache</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

      <p>The <strong>call</strong> function is the main function of a JAX module.
It defines how the module behaves when called as a function, and it's what you'll use to call your model in practice.
The <strong>call</strong> method takes an input tensor (x) and returns an output tensor (y).
In this case, we're defining our model to be a simple linear layer with no activation: y = x @ w + b.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>self</code></td>
            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Refer to the object itself</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>hidden_states</code></td>
            <td>
                  <code><span title="chex.Array">Array</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>chex.Array: Pass in the hidden state of the
model</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>freq_cis</code></td>
            <td>
                  <code><span title="typing.Tuple">Tuple</span>[<span title="chex.Array">Array</span>, <span title="chex.Array">Array</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Tuple[chex.Array, chex.Array],: Create the
apply_rotary variable</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>attention_mask</code></td>
            <td>
                  <code><span title="chex.Array">Array</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>chex.Array: Mask the attention weights</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>causal_mask</code></td>
            <td>
                  <code><span title="chex.Array">Array</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>chex.Array: Mask the attention weights</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>position_ids</code></td>
            <td>
                  <code><span title="chex.Array">Array</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>chex.Array: Specify the position of each token
in a sequence</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>deterministic</code></td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>bool: Determine whether to use dropout or not</p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>init_cache</code></td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>bool: Initialize the cache</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>output_attentions</code></td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>bool: Determine whether to return the
attention weights</p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>A tuple of (out, attn_output)</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>src/python/easydel/modules/mixtral/modelling_mixtral_flax.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">:</span> <span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span>
        <span class="n">freq_cis</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span> <span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">],</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span>
        <span class="n">causal_mask</span><span class="p">:</span> <span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span>
        <span class="n">position_ids</span><span class="p">:</span> <span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span>
        <span class="n">segment_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">deterministic</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">init_cache</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;The __call__ function is the main function of a JAX module.</span>
<span class="sd">    It defines how the module behaves when called as a function, and it&#39;s what you&#39;ll use to call your model in practice.</span>
<span class="sd">    The __call__ method takes an input tensor (x) and returns an output tensor (y).</span>
<span class="sd">    In this case, we&#39;re defining our model to be a simple linear layer with no activation: y = x @ w + b.</span>

<span class="sd">    Args:</span>
<span class="sd">        self: Refer to the object itself</span>
<span class="sd">        hidden_states: chex.Array: Pass in the hidden state of the</span>
<span class="sd">            model</span>
<span class="sd">        freq_cis: Tuple[chex.Array, chex.Array],: Create the</span>
<span class="sd">            apply_rotary variable</span>
<span class="sd">        attention_mask: chex.Array: Mask the attention weights</span>
<span class="sd">        causal_mask: chex.Array: Mask the attention weights</span>
<span class="sd">        position_ids: chex.Array: Specify the position of each token</span>
<span class="sd">            in a sequence</span>
<span class="sd">        deterministic: bool: Determine whether to use dropout or not</span>
<span class="sd">        init_cache: bool: Initialize the cache</span>
<span class="sd">        output_attentions: bool: Determine whether to return the</span>
<span class="sd">            attention weights</span>

<span class="sd">    Returns:</span>
<span class="sd">        A tuple of (out, attn_output)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">query_states</span><span class="p">,</span> <span class="n">key_states</span><span class="p">,</span> <span class="n">value_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span><span class="p">(</span>
        <span class="n">hidden_states</span><span class="p">)</span>

    <span class="n">query_states</span> <span class="o">=</span> <span class="n">query_states</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>
    <span class="n">key_states</span> <span class="o">=</span> <span class="n">key_states</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_key_value_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>
    <span class="n">value_states</span> <span class="o">=</span> <span class="n">value_states</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_key_value_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>

    <span class="n">query_states</span><span class="p">,</span> <span class="n">key_states</span><span class="p">,</span> <span class="n">value_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_rotary</span><span class="p">(</span>
        <span class="n">query</span><span class="o">=</span><span class="n">query_states</span><span class="p">,</span>
        <span class="n">key</span><span class="o">=</span><span class="n">key_states</span><span class="p">,</span>
        <span class="n">value</span><span class="o">=</span><span class="n">value_states</span><span class="p">,</span>
        <span class="n">position_ids</span><span class="o">=</span><span class="n">position_ids</span><span class="p">,</span>
        <span class="n">freq_cis</span><span class="o">=</span><span class="n">freq_cis</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
        <span class="n">sequence_length</span><span class="o">=</span><span class="n">sequence_length</span>
    <span class="p">)</span>

    <span class="n">assert_msg</span> <span class="o">=</span> <span class="p">(</span>
        <span class="s2">&quot;num_attention_heads repeat wont work likely</span><span class="se">\n</span><span class="s2">&quot;</span>
        <span class="sa">f</span><span class="s2">&quot;INFO :</span><span class="se">\n\t</span><span class="s2">repeat_kv_bnsh Used with num_key_value_groups = </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">num_key_value_groups</span><span class="si">}</span><span class="se">\n\t</span><span class="s2">&quot;</span>
        <span class="sa">f</span><span class="s2">&quot;NH : </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="si">}</span><span class="s2"> KVH : </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>

    <span class="k">assert</span> <span class="n">query_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="p">,</span> <span class="n">assert_msg</span>
    <span class="k">assert</span> <span class="n">key_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="p">,</span> <span class="n">assert_msg</span>
    <span class="k">assert</span> <span class="n">value_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="p">,</span> <span class="n">assert_msg</span>

    <span class="n">query_length</span><span class="p">,</span> <span class="n">key_length</span> <span class="o">=</span> <span class="n">query_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">key_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_variable</span><span class="p">(</span><span class="s2">&quot;cache&quot;</span><span class="p">,</span> <span class="s2">&quot;cached_key&quot;</span><span class="p">):</span>
        <span class="n">mask_shift</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">variables</span><span class="p">[</span><span class="s2">&quot;cache&quot;</span><span class="p">][</span><span class="s2">&quot;cache_index&quot;</span><span class="p">]</span>
        <span class="n">max_decoder_length</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">variables</span><span class="p">[</span><span class="s2">&quot;cache&quot;</span><span class="p">][</span><span class="s2">&quot;cached_key&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">causal_mask</span> <span class="o">=</span> <span class="n">lax</span><span class="o">.</span><span class="n">dynamic_slice</span><span class="p">(</span>
            <span class="n">causal_mask</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">mask_shift</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
                                                 <span class="n">query_length</span><span class="p">,</span> <span class="n">max_decoder_length</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">causal_mask</span> <span class="o">=</span> <span class="n">causal_mask</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:</span><span class="n">query_length</span><span class="p">,</span> <span class="p">:</span><span class="n">key_length</span><span class="p">]</span>

    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">causal_mask</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span>
        <span class="n">causal_mask</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,)</span> <span class="o">+</span> <span class="n">causal_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
    <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span>
        <span class="n">attention_mask</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)),</span> <span class="n">causal_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">combine_masks</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">causal_mask</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span>

    <span class="n">dropout_rng</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">deterministic</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">attention_dropout</span> <span class="o">&gt;</span> <span class="mf">0.0</span><span class="p">:</span>
        <span class="n">dropout_rng</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">make_rng</span><span class="p">(</span><span class="s2">&quot;dropout&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_variable</span><span class="p">(</span><span class="s2">&quot;cache&quot;</span><span class="p">,</span> <span class="s2">&quot;cached_key&quot;</span><span class="p">)</span> <span class="ow">or</span> <span class="n">init_cache</span><span class="p">:</span>
        <span class="n">key_states</span><span class="p">,</span> <span class="n">value_states</span><span class="p">,</span> <span class="n">attention_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_concatenate_to_cache</span><span class="p">(</span>
            <span class="n">key_states</span><span class="p">,</span>
            <span class="n">value_states</span><span class="p">,</span>
            <span class="n">query_states</span><span class="p">,</span>
            <span class="n">attention_mask</span>
        <span class="p">)</span>
    <span class="c1"># if self.config.use_sharding_constraint:</span>
    <span class="c1">#     query_states = with_sharding_constraint(</span>
    <span class="c1">#         query_states, PartitionSpec((&quot;dp&quot;, &quot;fsdp&quot;), &quot;sp&quot; if query_states.shape[1] != 1 else None, &quot;tp&quot;, None)</span>
    <span class="c1">#     )</span>
    <span class="c1">#     key_states = with_sharding_constraint(</span>
    <span class="c1">#         key_states, PartitionSpec((&quot;dp&quot;, &quot;fsdp&quot;), &quot;sp&quot;, &quot;tp&quot;, None)</span>
    <span class="c1">#     )</span>
    <span class="c1">#     value_states = with_sharding_constraint(</span>
    <span class="c1">#         value_states, PartitionSpec((&quot;dp&quot;, &quot;fsdp&quot;), &quot;sp&quot;, &quot;tp&quot;, None)</span>
    <span class="c1">#     )</span>
    <span class="n">attention_bias</span> <span class="o">=</span> <span class="n">lax</span><span class="o">.</span><span class="n">select</span><span class="p">(</span>
        <span class="n">attention_mask</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">jnp</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="n">attention_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span>
        <span class="n">jnp</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="n">attention_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">min</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="n">query_length</span><span class="p">,</span> <span class="n">key_length</span> <span class="o">=</span> <span class="n">query_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">key_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="n">attentions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_performer</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span>
        <span class="n">query_states</span><span class="o">=</span><span class="n">query_states</span><span class="p">,</span>
        <span class="n">key_states</span><span class="o">=</span><span class="n">key_states</span><span class="p">,</span>
        <span class="n">value_states</span><span class="o">=</span><span class="n">value_states</span><span class="p">,</span>
        <span class="n">bias</span><span class="o">=</span><span class="n">attention_bias</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
        <span class="n">causal</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">dropout_rng</span><span class="o">=</span><span class="n">dropout_rng</span><span class="p">,</span>
        <span class="n">deterministic</span><span class="o">=</span><span class="n">deterministic</span><span class="p">,</span>
        <span class="n">query_sequence_length</span><span class="o">=</span><span class="n">query_length</span><span class="p">,</span>
        <span class="n">key_value_sequence_length</span><span class="o">=</span><span class="n">key_length</span><span class="p">,</span>
        <span class="n">uses_cache</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">has_variable</span><span class="p">(</span><span class="s2">&quot;cache&quot;</span><span class="p">,</span> <span class="s2">&quot;cached_key&quot;</span><span class="p">)</span> <span class="ow">or</span> <span class="n">init_cache</span><span class="p">,</span>
        <span class="n">segment_ids</span><span class="o">=</span><span class="n">segment_ids</span><span class="p">,</span>
        <span class="n">causal_mask</span><span class="o">=</span><span class="n">causal_mask</span>
    <span class="p">)</span>

    <span class="n">attn_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_merge_heads</span><span class="p">(</span><span class="n">attentions</span><span class="o">.</span><span class="n">attention_outputs</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">shard_attention_computation</span><span class="p">:</span>
        <span class="n">attn_output</span> <span class="o">=</span> <span class="n">with_sharding_constraint</span><span class="p">(</span>
            <span class="n">attn_output</span><span class="p">,</span> <span class="n">PartitionSpec</span><span class="p">(</span>
                <span class="p">(</span><span class="s2">&quot;dp&quot;</span><span class="p">,</span> <span class="s2">&quot;fsdp&quot;</span><span class="p">),</span>
                <span class="s2">&quot;sp&quot;</span> <span class="k">if</span> <span class="n">attn_output</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">1</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
                <span class="s2">&quot;tp&quot;</span>
            <span class="p">)</span>
        <span class="p">)</span>
    <span class="n">attn_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">o_proj</span><span class="p">(</span><span class="n">attn_output</span><span class="p">)</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">attn_output</span><span class="p">,</span> <span class="n">attentions</span><span class="o">.</span><span class="n">attention_weights</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">outputs</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="src.python.easydel.modules.mixtral.modelling_mixtral_flax.FlaxMixtralDecoderLayer" class="doc doc-heading">
            <code>FlaxMixtralDecoderLayer</code>


</h2>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="flax.linen.Module">Module</span></code></p>


              <details class="quote">
                <summary>Source code in <code>src/python/easydel/modules/mixtral/modelling_mixtral_flax.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">FlaxMixtralDecoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="n">config</span><span class="p">:</span> <span class="n">MixtralConfig</span>
    <span class="n">layer_index</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">dtype</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">bfloat16</span>
    <span class="n">param_dtype</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">bfloat16</span>
    <span class="n">precision</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">Precision</span><span class="p">]]</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">Precision</span><span class="p">(</span><span class="s2">&quot;fastest&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">setup</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># hidden_states: chex.Array</span>
        <span class="c1"># freq_cis: Tuple[chex.Array, chex.Array],</span>
        <span class="c1"># attention_mask: chex.Array</span>
        <span class="c1"># causal_mask: chex.Array</span>
        <span class="c1"># position_ids: chex.Array</span>
        <span class="c1"># deterministic: bool = True</span>
        <span class="c1"># init_cache: bool = False</span>
        <span class="c1"># output_attentions: bool = True</span>

        <span class="n">attn_block</span> <span class="o">=</span> <span class="n">FlaxMixtralAttention</span>
        <span class="n">mlp_block</span> <span class="o">=</span> <span class="n">FlaxMixtralSparseMoeBlock</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">gradient_checkpointing</span> <span class="o">!=</span> <span class="s2">&quot;&quot;</span><span class="p">:</span>
            <span class="n">attn_block</span> <span class="o">=</span> <span class="n">re_mat</span><span class="p">(</span>
                <span class="n">attn_block</span><span class="p">,</span>
                <span class="n">policy</span><span class="o">=</span><span class="n">get_gradient_checkpoint_policy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">gradient_checkpointing</span><span class="p">),</span>
                <span class="n">static_argnums</span><span class="o">=</span><span class="p">(</span>
                    <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span>
                <span class="p">)</span>
            <span class="p">)</span>
            <span class="n">mlp_block</span> <span class="o">=</span> <span class="n">re_mat</span><span class="p">(</span>
                <span class="n">mlp_block</span><span class="p">,</span>
                <span class="n">policy</span><span class="o">=</span><span class="n">get_gradient_checkpoint_policy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">gradient_checkpointing</span><span class="p">),</span>
                <span class="n">static_argnums</span><span class="o">=</span><span class="p">(</span>
                    <span class="mi">1</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span> <span class="o">=</span> <span class="n">attn_block</span><span class="p">(</span>
            <span class="n">config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">,</span>
            <span class="n">layer_index</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_index</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">param_dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">param_dtype</span><span class="p">,</span>
            <span class="n">precision</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">precision</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">block_sparse_moe</span> <span class="o">=</span> <span class="n">mlp_block</span><span class="p">(</span>
            <span class="n">config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">param_dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">param_dtype</span><span class="p">,</span>
            <span class="n">precision</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">precision</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_layernorm</span> <span class="o">=</span> <span class="n">MixtralRMSNorm</span><span class="p">(</span>
            <span class="n">dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span>
            <span class="n">eps</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">rms_norm_eps</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">param_dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">param_dtype</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">post_attention_layernorm</span> <span class="o">=</span> <span class="n">MixtralRMSNorm</span><span class="p">(</span>
            <span class="n">dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span>
            <span class="n">eps</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">rms_norm_eps</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">param_dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">param_dtype</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">hidden_states</span><span class="p">:</span> <span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span>
            <span class="n">freq_cis</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span> <span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">],</span>
            <span class="n">attention_mask</span><span class="p">:</span> <span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span>
            <span class="n">causal_mask</span><span class="p">:</span> <span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span>
            <span class="n">position_ids</span><span class="p">:</span> <span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span>
            <span class="n">segment_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">deterministic</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
            <span class="n">init_cache</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
            <span class="n">output_attentions</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
            <span class="n">output_router_logits</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;The __call__ function is the main function of a TransformerEncoderLayer.</span>
<span class="sd">        It takes in the following arguments:</span>
<span class="sd">            hidden_states (chex.Array): The input to the encoder layer, which is also its output after being processed by all sublayers.</span>
<span class="sd">            freq_cis (chex.Array): A tensor containing frequency-domain representations of each token&#39;s context vector, used for computing self-attention weights and biases in a more efficient manner than using position embeddings or sinusoidal positional encoding vectors would allow for [2]. This tensor has shape `(batch_size, num</span>

<span class="sd">        Args:</span>
<span class="sd">            self: Represent the instance of the class</span>
<span class="sd">            hidden_states: chex.Array: Represent the input to the</span>
<span class="sd">                encoder layer</span>
<span class="sd">            freq_cis: Tuple[chex.Array, chex.Array],: Pass the frequency</span>
<span class="sd">                information to the attention layer</span>
<span class="sd">            attention_mask: chex.Array: Mask out the attention weights</span>
<span class="sd">                for certain positions</span>
<span class="sd">            causal_mask: chex.Array: Mask the future tokens</span>
<span class="sd">            position_ids: chex.Array: Indicate the position of each</span>
<span class="sd">                token in the sequence</span>
<span class="sd">            deterministic: bool: Determine whether to use dropout or not</span>
<span class="sd">            init_cache: bool: Initialize the cache for the self-</span>
<span class="sd">                attention layer</span>
<span class="sd">            output_attentions: bool: Determine whether to return the</span>
<span class="sd">                attention weights or not</span>

<span class="sd">        Returns:</span>
<span class="sd">            A tuple of hidden_states and attention_output</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_layernorm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

        <span class="c1"># hidden_states: chex.Array</span>
        <span class="c1"># freq_cis: Tuple[chex.Array, chex.Array],</span>
        <span class="c1"># attention_mask: chex.Array</span>
        <span class="c1"># causal_mask: chex.Array</span>
        <span class="c1"># position_ids: chex.Array</span>
        <span class="c1"># segment_ids: Optional[chex.Array] = None</span>
        <span class="c1"># deterministic: bool = True</span>
        <span class="c1"># init_cache: bool = False</span>
        <span class="c1"># output_attentions: bool = True</span>

        <span class="n">hidden_states</span><span class="p">,</span> <span class="n">self_attn_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span><span class="p">(</span>
            <span class="n">hidden_states</span><span class="p">,</span>
            <span class="n">freq_cis</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="p">,</span>
            <span class="n">causal_mask</span><span class="p">,</span>
            <span class="n">position_ids</span><span class="p">,</span>
            <span class="n">segment_ids</span><span class="p">,</span>
            <span class="n">deterministic</span><span class="p">,</span>
            <span class="n">init_cache</span><span class="p">,</span>
            <span class="n">output_attentions</span>
        <span class="p">)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="n">hidden_states</span>

        <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">post_attention_layernorm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">hidden_states</span><span class="p">,</span> <span class="n">router_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_sparse_moe</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="n">hidden_states</span>

        <span class="n">outputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">hidden_states</span><span class="p">,)</span>
        <span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
            <span class="n">outputs</span> <span class="o">+=</span> <span class="p">(</span><span class="n">self_attn_weights</span><span class="p">,)</span>
        <span class="k">if</span> <span class="n">output_router_logits</span><span class="p">:</span>
            <span class="n">outputs</span> <span class="o">+=</span> <span class="p">(</span><span class="n">router_logits</span><span class="p">,)</span>
        <span class="k">return</span> <span class="n">outputs</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="src.python.easydel.modules.mixtral.modelling_mixtral_flax.FlaxMixtralDecoderLayer.__call__" class="doc doc-heading">
            <code class="highlight language-python"><span class="fm">__call__</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">freq_cis</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">causal_mask</span><span class="p">,</span> <span class="n">position_ids</span><span class="p">,</span> <span class="n">segment_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">deterministic</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">init_cache</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">output_router_logits</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

      <p>The <strong>call</strong> function is the main function of a TransformerEncoderLayer.
It takes in the following arguments:
    hidden_states (chex.Array): The input to the encoder layer, which is also its output after being processed by all sublayers.
    freq_cis (chex.Array): A tensor containing frequency-domain representations of each token's context vector, used for computing self-attention weights and biases in a more efficient manner than using position embeddings or sinusoidal positional encoding vectors would allow for [2]. This tensor has shape `(batch_size, num</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>self</code></td>
            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Represent the instance of the class</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>hidden_states</code></td>
            <td>
                  <code><span title="chex.Array">Array</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>chex.Array: Represent the input to the
encoder layer</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>freq_cis</code></td>
            <td>
                  <code><span title="typing.Tuple">Tuple</span>[<span title="chex.Array">Array</span>, <span title="chex.Array">Array</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Tuple[chex.Array, chex.Array],: Pass the frequency
information to the attention layer</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>attention_mask</code></td>
            <td>
                  <code><span title="chex.Array">Array</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>chex.Array: Mask out the attention weights
for certain positions</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>causal_mask</code></td>
            <td>
                  <code><span title="chex.Array">Array</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>chex.Array: Mask the future tokens</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>position_ids</code></td>
            <td>
                  <code><span title="chex.Array">Array</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>chex.Array: Indicate the position of each
token in the sequence</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>deterministic</code></td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>bool: Determine whether to use dropout or not</p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>init_cache</code></td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>bool: Initialize the cache for the self-
attention layer</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>output_attentions</code></td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>bool: Determine whether to return the
attention weights or not</p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>A tuple of hidden_states and attention_output</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>src/python/easydel/modules/mixtral/modelling_mixtral_flax.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">:</span> <span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span>
        <span class="n">freq_cis</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span> <span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">],</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span>
        <span class="n">causal_mask</span><span class="p">:</span> <span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span>
        <span class="n">position_ids</span><span class="p">:</span> <span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span>
        <span class="n">segment_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">deterministic</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">init_cache</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">output_router_logits</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;The __call__ function is the main function of a TransformerEncoderLayer.</span>
<span class="sd">    It takes in the following arguments:</span>
<span class="sd">        hidden_states (chex.Array): The input to the encoder layer, which is also its output after being processed by all sublayers.</span>
<span class="sd">        freq_cis (chex.Array): A tensor containing frequency-domain representations of each token&#39;s context vector, used for computing self-attention weights and biases in a more efficient manner than using position embeddings or sinusoidal positional encoding vectors would allow for [2]. This tensor has shape `(batch_size, num</span>

<span class="sd">    Args:</span>
<span class="sd">        self: Represent the instance of the class</span>
<span class="sd">        hidden_states: chex.Array: Represent the input to the</span>
<span class="sd">            encoder layer</span>
<span class="sd">        freq_cis: Tuple[chex.Array, chex.Array],: Pass the frequency</span>
<span class="sd">            information to the attention layer</span>
<span class="sd">        attention_mask: chex.Array: Mask out the attention weights</span>
<span class="sd">            for certain positions</span>
<span class="sd">        causal_mask: chex.Array: Mask the future tokens</span>
<span class="sd">        position_ids: chex.Array: Indicate the position of each</span>
<span class="sd">            token in the sequence</span>
<span class="sd">        deterministic: bool: Determine whether to use dropout or not</span>
<span class="sd">        init_cache: bool: Initialize the cache for the self-</span>
<span class="sd">            attention layer</span>
<span class="sd">        output_attentions: bool: Determine whether to return the</span>
<span class="sd">            attention weights or not</span>

<span class="sd">    Returns:</span>
<span class="sd">        A tuple of hidden_states and attention_output</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>
    <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_layernorm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>

    <span class="c1"># hidden_states: chex.Array</span>
    <span class="c1"># freq_cis: Tuple[chex.Array, chex.Array],</span>
    <span class="c1"># attention_mask: chex.Array</span>
    <span class="c1"># causal_mask: chex.Array</span>
    <span class="c1"># position_ids: chex.Array</span>
    <span class="c1"># segment_ids: Optional[chex.Array] = None</span>
    <span class="c1"># deterministic: bool = True</span>
    <span class="c1"># init_cache: bool = False</span>
    <span class="c1"># output_attentions: bool = True</span>

    <span class="n">hidden_states</span><span class="p">,</span> <span class="n">self_attn_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span><span class="p">(</span>
        <span class="n">hidden_states</span><span class="p">,</span>
        <span class="n">freq_cis</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">,</span>
        <span class="n">causal_mask</span><span class="p">,</span>
        <span class="n">position_ids</span><span class="p">,</span>
        <span class="n">segment_ids</span><span class="p">,</span>
        <span class="n">deterministic</span><span class="p">,</span>
        <span class="n">init_cache</span><span class="p">,</span>
        <span class="n">output_attentions</span>
    <span class="p">)</span>

    <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="n">hidden_states</span>

    <span class="n">residual</span> <span class="o">=</span> <span class="n">hidden_states</span>
    <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">post_attention_layernorm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
    <span class="n">hidden_states</span><span class="p">,</span> <span class="n">router_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_sparse_moe</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
    <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="n">hidden_states</span>

    <span class="n">outputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">hidden_states</span><span class="p">,)</span>
    <span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
        <span class="n">outputs</span> <span class="o">+=</span> <span class="p">(</span><span class="n">self_attn_weights</span><span class="p">,)</span>
    <span class="k">if</span> <span class="n">output_router_logits</span><span class="p">:</span>
        <span class="n">outputs</span> <span class="o">+=</span> <span class="p">(</span><span class="n">router_logits</span><span class="p">,)</span>
    <span class="k">return</span> <span class="n">outputs</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="src.python.easydel.modules.mixtral.modelling_mixtral_flax.FlaxMixtralDecoderLayerCollection" class="doc doc-heading">
            <code>FlaxMixtralDecoderLayerCollection</code>


</h2>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="flax.linen.Module">Module</span></code></p>


              <details class="quote">
                <summary>Source code in <code>src/python/easydel/modules/mixtral/modelling_mixtral_flax.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span>
<span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span>
<span class="normal">639</span>
<span class="normal">640</span>
<span class="normal">641</span>
<span class="normal">642</span>
<span class="normal">643</span>
<span class="normal">644</span>
<span class="normal">645</span>
<span class="normal">646</span>
<span class="normal">647</span>
<span class="normal">648</span>
<span class="normal">649</span>
<span class="normal">650</span>
<span class="normal">651</span>
<span class="normal">652</span>
<span class="normal">653</span>
<span class="normal">654</span>
<span class="normal">655</span>
<span class="normal">656</span>
<span class="normal">657</span>
<span class="normal">658</span>
<span class="normal">659</span>
<span class="normal">660</span>
<span class="normal">661</span>
<span class="normal">662</span>
<span class="normal">663</span>
<span class="normal">664</span>
<span class="normal">665</span>
<span class="normal">666</span>
<span class="normal">667</span>
<span class="normal">668</span>
<span class="normal">669</span>
<span class="normal">670</span>
<span class="normal">671</span>
<span class="normal">672</span>
<span class="normal">673</span>
<span class="normal">674</span>
<span class="normal">675</span>
<span class="normal">676</span>
<span class="normal">677</span>
<span class="normal">678</span>
<span class="normal">679</span>
<span class="normal">680</span>
<span class="normal">681</span>
<span class="normal">682</span>
<span class="normal">683</span>
<span class="normal">684</span>
<span class="normal">685</span>
<span class="normal">686</span>
<span class="normal">687</span>
<span class="normal">688</span>
<span class="normal">689</span>
<span class="normal">690</span>
<span class="normal">691</span>
<span class="normal">692</span>
<span class="normal">693</span>
<span class="normal">694</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">FlaxMixtralDecoderLayerCollection</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="n">config</span><span class="p">:</span> <span class="n">MixtralConfig</span>
    <span class="n">dtype</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">bfloat16</span>
    <span class="n">param_dtype</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">bfloat16</span>
    <span class="n">precision</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">Precision</span><span class="p">]</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">Precision</span><span class="p">(</span><span class="s2">&quot;fastest&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">setup</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">FlaxMixtralDecoderLayer</span><span class="p">(</span>
                <span class="n">layer_index</span><span class="o">=</span><span class="n">layer_index</span><span class="p">,</span>
                <span class="n">config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">,</span>
                <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                <span class="n">param_dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">param_dtype</span><span class="p">,</span>
                <span class="n">precision</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">precision</span><span class="p">,</span>
                <span class="n">name</span><span class="o">=</span><span class="nb">str</span><span class="p">(</span><span class="n">layer_index</span><span class="p">)</span>
            <span class="p">)</span>

            <span class="k">for</span> <span class="n">layer_index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_hidden_layers</span><span class="p">)</span>
        <span class="p">]</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">hidden_states</span><span class="p">:</span> <span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span>
            <span class="n">freq_cis</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span> <span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">],</span>
            <span class="n">attention_mask</span><span class="p">:</span> <span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span>
            <span class="n">causal_mask</span><span class="p">:</span> <span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span>
            <span class="n">position_ids</span><span class="p">:</span> <span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span>
            <span class="n">deterministic</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
            <span class="n">init_cache</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
            <span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
            <span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
            <span class="n">output_router_logits</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;The __call__ function is the main function of a TransformerEncoderLayer.</span>
<span class="sd">        It takes in the following arguments:</span>
<span class="sd">            hidden_states (chex.Array): The input to the encoder layer, which is also its output after being processed by all sublayers.</span>
<span class="sd">            freq_cis (chex.Array): A tensor containing frequency-domain representations of each token&#39;s context vector, used for computing self-attention weights and biases in a more efficient manner than using position embeddings or sinusoidal positional encoding vectors would allow for [2]. This tensor has shape `(batch_size, num</span>

<span class="sd">        Args:</span>
<span class="sd">            self: Represent the instance of the class</span>
<span class="sd">            hidden_states: chex.Array: Represent the input to the</span>
<span class="sd">                encoder layer</span>
<span class="sd">            freq_cis: Tuple[chex.Array, chex.Array],: Pass the frequency</span>
<span class="sd">                information to the attention layer</span>
<span class="sd">            attention_mask: chex.Array: Mask out the attention weights</span>
<span class="sd">                for certain positions</span>
<span class="sd">            causal_mask: chex.Array: Mask the future tokens</span>
<span class="sd">            position_ids: chex.Array: Indicate the position of each</span>
<span class="sd">                token in the sequence</span>
<span class="sd">            deterministic: bool: Determine whether to use dropout or not</span>
<span class="sd">            init_cache: bool: Initialize the cache for the self-</span>
<span class="sd">                attention layer</span>
<span class="sd">            output_attentions: bool: Determine whether to return the</span>
<span class="sd">                attention weights or not</span>

<span class="sd">        Returns:</span>
<span class="sd">            A tuple of hidden_states, attention_output,</span>
<span class="sd">            all_hidden_states and all_router_logits</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">all_hidden_states</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">all_self_attns</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">all_router_logits</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">output_router_logits</span> <span class="k">else</span> <span class="kc">None</span>

        <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">output_hidden_states</span><span class="p">:</span>
                <span class="n">all_hidden_states</span> <span class="o">+=</span> <span class="p">(</span><span class="n">hidden_states</span><span class="p">,)</span>
            <span class="n">layer_outputs</span> <span class="o">=</span> <span class="n">block</span><span class="p">(</span>
                <span class="n">hidden_states</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">,</span>
                <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
                <span class="n">position_ids</span><span class="o">=</span><span class="n">position_ids</span><span class="p">,</span>
                <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
                <span class="n">output_router_logits</span><span class="o">=</span><span class="n">output_router_logits</span><span class="p">,</span>
                <span class="n">init_cache</span><span class="o">=</span><span class="n">init_cache</span><span class="p">,</span>
                <span class="n">freq_cis</span><span class="o">=</span><span class="n">freq_cis</span><span class="p">,</span>
                <span class="n">causal_mask</span><span class="o">=</span><span class="n">causal_mask</span><span class="p">,</span>
                <span class="n">deterministic</span><span class="o">=</span><span class="n">deterministic</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">layer_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

            <span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
                <span class="n">all_self_attns</span> <span class="o">+=</span> <span class="p">(</span><span class="n">layer_outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">],)</span>

            <span class="k">if</span> <span class="n">output_router_logits</span><span class="p">:</span>
                <span class="n">all_router_logits</span> <span class="o">+=</span> <span class="p">(</span><span class="n">layer_outputs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],)</span>

        <span class="n">outputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">hidden_states</span><span class="p">,)</span>
        <span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
            <span class="n">outputs</span> <span class="o">+=</span> <span class="p">(</span><span class="n">all_self_attns</span><span class="p">,)</span>
        <span class="k">if</span> <span class="n">output_hidden_states</span><span class="p">:</span>
            <span class="n">outputs</span> <span class="o">+=</span> <span class="p">(</span><span class="n">all_hidden_states</span><span class="p">,)</span>
        <span class="k">if</span> <span class="n">output_router_logits</span><span class="p">:</span>
            <span class="n">outputs</span> <span class="o">+=</span> <span class="p">(</span><span class="n">all_router_logits</span><span class="p">,)</span>
        <span class="k">return</span> <span class="n">outputs</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="src.python.easydel.modules.mixtral.modelling_mixtral_flax.FlaxMixtralDecoderLayerCollection.__call__" class="doc doc-heading">
            <code class="highlight language-python"><span class="fm">__call__</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">freq_cis</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">causal_mask</span><span class="p">,</span> <span class="n">position_ids</span><span class="p">,</span> <span class="n">deterministic</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">init_cache</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">output_hidden_states</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">output_router_logits</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

      <p>The <strong>call</strong> function is the main function of a TransformerEncoderLayer.
It takes in the following arguments:
    hidden_states (chex.Array): The input to the encoder layer, which is also its output after being processed by all sublayers.
    freq_cis (chex.Array): A tensor containing frequency-domain representations of each token's context vector, used for computing self-attention weights and biases in a more efficient manner than using position embeddings or sinusoidal positional encoding vectors would allow for [2]. This tensor has shape `(batch_size, num</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>self</code></td>
            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Represent the instance of the class</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>hidden_states</code></td>
            <td>
                  <code><span title="chex.Array">Array</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>chex.Array: Represent the input to the
encoder layer</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>freq_cis</code></td>
            <td>
                  <code><span title="typing.Tuple">Tuple</span>[<span title="chex.Array">Array</span>, <span title="chex.Array">Array</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Tuple[chex.Array, chex.Array],: Pass the frequency
information to the attention layer</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>attention_mask</code></td>
            <td>
                  <code><span title="chex.Array">Array</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>chex.Array: Mask out the attention weights
for certain positions</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>causal_mask</code></td>
            <td>
                  <code><span title="chex.Array">Array</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>chex.Array: Mask the future tokens</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>position_ids</code></td>
            <td>
                  <code><span title="chex.Array">Array</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>chex.Array: Indicate the position of each
token in the sequence</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>deterministic</code></td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>bool: Determine whether to use dropout or not</p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>init_cache</code></td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>bool: Initialize the cache for the self-
attention layer</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>output_attentions</code></td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[bool]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>bool: Determine whether to return the
attention weights or not</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>A tuple of hidden_states, attention_output,</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>all_hidden_states and all_router_logits</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>src/python/easydel/modules/mixtral/modelling_mixtral_flax.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span>
<span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span>
<span class="normal">639</span>
<span class="normal">640</span>
<span class="normal">641</span>
<span class="normal">642</span>
<span class="normal">643</span>
<span class="normal">644</span>
<span class="normal">645</span>
<span class="normal">646</span>
<span class="normal">647</span>
<span class="normal">648</span>
<span class="normal">649</span>
<span class="normal">650</span>
<span class="normal">651</span>
<span class="normal">652</span>
<span class="normal">653</span>
<span class="normal">654</span>
<span class="normal">655</span>
<span class="normal">656</span>
<span class="normal">657</span>
<span class="normal">658</span>
<span class="normal">659</span>
<span class="normal">660</span>
<span class="normal">661</span>
<span class="normal">662</span>
<span class="normal">663</span>
<span class="normal">664</span>
<span class="normal">665</span>
<span class="normal">666</span>
<span class="normal">667</span>
<span class="normal">668</span>
<span class="normal">669</span>
<span class="normal">670</span>
<span class="normal">671</span>
<span class="normal">672</span>
<span class="normal">673</span>
<span class="normal">674</span>
<span class="normal">675</span>
<span class="normal">676</span>
<span class="normal">677</span>
<span class="normal">678</span>
<span class="normal">679</span>
<span class="normal">680</span>
<span class="normal">681</span>
<span class="normal">682</span>
<span class="normal">683</span>
<span class="normal">684</span>
<span class="normal">685</span>
<span class="normal">686</span>
<span class="normal">687</span>
<span class="normal">688</span>
<span class="normal">689</span>
<span class="normal">690</span>
<span class="normal">691</span>
<span class="normal">692</span>
<span class="normal">693</span>
<span class="normal">694</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">hidden_states</span><span class="p">:</span> <span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span>
        <span class="n">freq_cis</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span> <span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">],</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span>
        <span class="n">causal_mask</span><span class="p">:</span> <span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span>
        <span class="n">position_ids</span><span class="p">:</span> <span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span>
        <span class="n">deterministic</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">init_cache</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">output_router_logits</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;The __call__ function is the main function of a TransformerEncoderLayer.</span>
<span class="sd">    It takes in the following arguments:</span>
<span class="sd">        hidden_states (chex.Array): The input to the encoder layer, which is also its output after being processed by all sublayers.</span>
<span class="sd">        freq_cis (chex.Array): A tensor containing frequency-domain representations of each token&#39;s context vector, used for computing self-attention weights and biases in a more efficient manner than using position embeddings or sinusoidal positional encoding vectors would allow for [2]. This tensor has shape `(batch_size, num</span>

<span class="sd">    Args:</span>
<span class="sd">        self: Represent the instance of the class</span>
<span class="sd">        hidden_states: chex.Array: Represent the input to the</span>
<span class="sd">            encoder layer</span>
<span class="sd">        freq_cis: Tuple[chex.Array, chex.Array],: Pass the frequency</span>
<span class="sd">            information to the attention layer</span>
<span class="sd">        attention_mask: chex.Array: Mask out the attention weights</span>
<span class="sd">            for certain positions</span>
<span class="sd">        causal_mask: chex.Array: Mask the future tokens</span>
<span class="sd">        position_ids: chex.Array: Indicate the position of each</span>
<span class="sd">            token in the sequence</span>
<span class="sd">        deterministic: bool: Determine whether to use dropout or not</span>
<span class="sd">        init_cache: bool: Initialize the cache for the self-</span>
<span class="sd">            attention layer</span>
<span class="sd">        output_attentions: bool: Determine whether to return the</span>
<span class="sd">            attention weights or not</span>

<span class="sd">    Returns:</span>
<span class="sd">        A tuple of hidden_states, attention_output,</span>
<span class="sd">        all_hidden_states and all_router_logits</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">all_hidden_states</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="k">else</span> <span class="kc">None</span>
    <span class="n">all_self_attns</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="k">else</span> <span class="kc">None</span>
    <span class="n">all_router_logits</span> <span class="o">=</span> <span class="p">()</span> <span class="k">if</span> <span class="n">output_router_logits</span> <span class="k">else</span> <span class="kc">None</span>

    <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">output_hidden_states</span><span class="p">:</span>
            <span class="n">all_hidden_states</span> <span class="o">+=</span> <span class="p">(</span><span class="n">hidden_states</span><span class="p">,)</span>
        <span class="n">layer_outputs</span> <span class="o">=</span> <span class="n">block</span><span class="p">(</span>
            <span class="n">hidden_states</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
            <span class="n">position_ids</span><span class="o">=</span><span class="n">position_ids</span><span class="p">,</span>
            <span class="n">output_attentions</span><span class="o">=</span><span class="n">output_attentions</span><span class="p">,</span>
            <span class="n">output_router_logits</span><span class="o">=</span><span class="n">output_router_logits</span><span class="p">,</span>
            <span class="n">init_cache</span><span class="o">=</span><span class="n">init_cache</span><span class="p">,</span>
            <span class="n">freq_cis</span><span class="o">=</span><span class="n">freq_cis</span><span class="p">,</span>
            <span class="n">causal_mask</span><span class="o">=</span><span class="n">causal_mask</span><span class="p">,</span>
            <span class="n">deterministic</span><span class="o">=</span><span class="n">deterministic</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">layer_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
            <span class="n">all_self_attns</span> <span class="o">+=</span> <span class="p">(</span><span class="n">layer_outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">],)</span>

        <span class="k">if</span> <span class="n">output_router_logits</span><span class="p">:</span>
            <span class="n">all_router_logits</span> <span class="o">+=</span> <span class="p">(</span><span class="n">layer_outputs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],)</span>

    <span class="n">outputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">hidden_states</span><span class="p">,)</span>
    <span class="k">if</span> <span class="n">output_attentions</span><span class="p">:</span>
        <span class="n">outputs</span> <span class="o">+=</span> <span class="p">(</span><span class="n">all_self_attns</span><span class="p">,)</span>
    <span class="k">if</span> <span class="n">output_hidden_states</span><span class="p">:</span>
        <span class="n">outputs</span> <span class="o">+=</span> <span class="p">(</span><span class="n">all_hidden_states</span><span class="p">,)</span>
    <span class="k">if</span> <span class="n">output_router_logits</span><span class="p">:</span>
        <span class="n">outputs</span> <span class="o">+=</span> <span class="p">(</span><span class="n">all_router_logits</span><span class="p">,)</span>
    <span class="k">return</span> <span class="n">outputs</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="src.python.easydel.modules.mixtral.modelling_mixtral_flax.FlaxMixtralForCausalLM" class="doc doc-heading">
            <code>FlaxMixtralForCausalLM</code>


</h2>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="src.python.easydel.modules.mixtral.modelling_mixtral_flax.MixtralPreTrainedModel" href="#src.python.easydel.modules.mixtral.modelling_mixtral_flax.MixtralPreTrainedModel">MixtralPreTrainedModel</a></code></p>


              <details class="quote">
                <summary>Source code in <code>src/python/easydel/modules/mixtral/modelling_mixtral_flax.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1141</span>
<span class="normal">1142</span>
<span class="normal">1143</span>
<span class="normal">1144</span>
<span class="normal">1145</span>
<span class="normal">1146</span>
<span class="normal">1147</span>
<span class="normal">1148</span>
<span class="normal">1149</span>
<span class="normal">1150</span>
<span class="normal">1151</span>
<span class="normal">1152</span>
<span class="normal">1153</span>
<span class="normal">1154</span>
<span class="normal">1155</span>
<span class="normal">1156</span>
<span class="normal">1157</span>
<span class="normal">1158</span>
<span class="normal">1159</span>
<span class="normal">1160</span>
<span class="normal">1161</span>
<span class="normal">1162</span>
<span class="normal">1163</span>
<span class="normal">1164</span>
<span class="normal">1165</span>
<span class="normal">1166</span>
<span class="normal">1167</span>
<span class="normal">1168</span>
<span class="normal">1169</span>
<span class="normal">1170</span>
<span class="normal">1171</span>
<span class="normal">1172</span>
<span class="normal">1173</span>
<span class="normal">1174</span>
<span class="normal">1175</span>
<span class="normal">1176</span>
<span class="normal">1177</span>
<span class="normal">1178</span>
<span class="normal">1179</span>
<span class="normal">1180</span>
<span class="normal">1181</span>
<span class="normal">1182</span>
<span class="normal">1183</span>
<span class="normal">1184</span>
<span class="normal">1185</span>
<span class="normal">1186</span>
<span class="normal">1187</span>
<span class="normal">1188</span>
<span class="normal">1189</span>
<span class="normal">1190</span>
<span class="normal">1191</span>
<span class="normal">1192</span>
<span class="normal">1193</span>
<span class="normal">1194</span>
<span class="normal">1195</span>
<span class="normal">1196</span>
<span class="normal">1197</span>
<span class="normal">1198</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">FlaxMixtralForCausalLM</span><span class="p">(</span><span class="n">MixtralPreTrainedModel</span><span class="p">):</span>
    <span class="n">module_class</span> <span class="o">=</span> <span class="n">FlaxMixtralForCausalLMModule</span>

    <span class="k">def</span> <span class="nf">set_input_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">embed_tokens</span> <span class="o">=</span> <span class="n">value</span>

    <span class="k">def</span> <span class="nf">get_input_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">embed_tokens</span>

    <span class="k">def</span> <span class="nf">set_decoder</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">decoder</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">decoder</span>

    <span class="k">def</span> <span class="nf">get_decoder</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">model</span>

    <span class="k">def</span> <span class="nf">get_output_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">lm_head</span>

    <span class="k">def</span> <span class="nf">set_output_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_embeddings</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">lm_head</span> <span class="o">=</span> <span class="n">new_embeddings</span>

    <span class="k">def</span> <span class="nf">prepare_inputs_for_generation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">max_length</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;The prepare_inputs_for_generation function is used to prepare the inputs for a generation task.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: Access variables that belong to the class</span>
<span class="sd">            input_ids: Pass in the input tokens</span>
<span class="sd">            max_length: Set the length of the sequence to be generated</span>
<span class="sd">            attention_mask: Optional[chex.Array]: Mask the attention</span>
<span class="sd">                weights</span>

<span class="sd">        Returns:</span>
<span class="sd">            A dictionary of the past_key_values, attention_mask and</span>
<span class="sd">            position ids</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span>

        <span class="n">past_key_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_cache</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">max_length</span><span class="p">)</span>
        <span class="n">extended_attention_mask</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span>
            <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">max_length</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;i4&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">position_ids</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
            <span class="n">extended_attention_mask</span> <span class="o">=</span> <span class="n">lax</span><span class="o">.</span><span class="n">dynamic_update_slice</span><span class="p">(</span>
                <span class="n">extended_attention_mask</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">position_ids</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">seq_length</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;i4&quot;</span><span class="p">)[</span>
                                            <span class="kc">None</span><span class="p">,</span> <span class="p">:],</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">))</span>

        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;past_key_values&quot;</span><span class="p">:</span> <span class="n">past_key_values</span><span class="p">,</span>
            <span class="s2">&quot;attention_mask&quot;</span><span class="p">:</span> <span class="n">extended_attention_mask</span><span class="p">,</span>
            <span class="s2">&quot;position_ids&quot;</span><span class="p">:</span> <span class="n">position_ids</span><span class="p">,</span>
        <span class="p">}</span>

    <span class="k">def</span> <span class="nf">update_inputs_for_generation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_outputs</span><span class="p">,</span> <span class="n">model_kwargs</span><span class="p">):</span>
        <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;past_key_values&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">model_outputs</span><span class="o">.</span><span class="n">past_key_values</span>
        <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;position_ids&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">model_kwargs</span><span class="p">[</span><span class="s2">&quot;position_ids&quot;</span><span class="p">][:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:]</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="n">model_kwargs</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="src.python.easydel.modules.mixtral.modelling_mixtral_flax.FlaxMixtralForCausalLM.prepare_inputs_for_generation" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">prepare_inputs_for_generation</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">max_length</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

      <p>The prepare_inputs_for_generation function is used to prepare the inputs for a generation task.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>self</code></td>
            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Access variables that belong to the class</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>input_ids</code></td>
            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Pass in the input tokens</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>max_length</code></td>
            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Set the length of the sequence to be generated</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>attention_mask</code></td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="chex.Array">Array</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Optional[chex.Array]: Mask the attention
weights</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>A dictionary of the past_key_values, attention_mask and</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>position ids</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>src/python/easydel/modules/mixtral/modelling_mixtral_flax.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1162</span>
<span class="normal">1163</span>
<span class="normal">1164</span>
<span class="normal">1165</span>
<span class="normal">1166</span>
<span class="normal">1167</span>
<span class="normal">1168</span>
<span class="normal">1169</span>
<span class="normal">1170</span>
<span class="normal">1171</span>
<span class="normal">1172</span>
<span class="normal">1173</span>
<span class="normal">1174</span>
<span class="normal">1175</span>
<span class="normal">1176</span>
<span class="normal">1177</span>
<span class="normal">1178</span>
<span class="normal">1179</span>
<span class="normal">1180</span>
<span class="normal">1181</span>
<span class="normal">1182</span>
<span class="normal">1183</span>
<span class="normal">1184</span>
<span class="normal">1185</span>
<span class="normal">1186</span>
<span class="normal">1187</span>
<span class="normal">1188</span>
<span class="normal">1189</span>
<span class="normal">1190</span>
<span class="normal">1191</span>
<span class="normal">1192</span>
<span class="normal">1193</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">prepare_inputs_for_generation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">max_length</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;The prepare_inputs_for_generation function is used to prepare the inputs for a generation task.</span>

<span class="sd">    Args:</span>
<span class="sd">        self: Access variables that belong to the class</span>
<span class="sd">        input_ids: Pass in the input tokens</span>
<span class="sd">        max_length: Set the length of the sequence to be generated</span>
<span class="sd">        attention_mask: Optional[chex.Array]: Mask the attention</span>
<span class="sd">            weights</span>

<span class="sd">    Returns:</span>
<span class="sd">        A dictionary of the past_key_values, attention_mask and</span>
<span class="sd">        position ids</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span>

    <span class="n">past_key_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_cache</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">max_length</span><span class="p">)</span>
    <span class="n">extended_attention_mask</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span>
        <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">max_length</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;i4&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">position_ids</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
        <span class="n">extended_attention_mask</span> <span class="o">=</span> <span class="n">lax</span><span class="o">.</span><span class="n">dynamic_update_slice</span><span class="p">(</span>
            <span class="n">extended_attention_mask</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">position_ids</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">seq_length</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;i4&quot;</span><span class="p">)[</span>
                                        <span class="kc">None</span><span class="p">,</span> <span class="p">:],</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">))</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;past_key_values&quot;</span><span class="p">:</span> <span class="n">past_key_values</span><span class="p">,</span>
        <span class="s2">&quot;attention_mask&quot;</span><span class="p">:</span> <span class="n">extended_attention_mask</span><span class="p">,</span>
        <span class="s2">&quot;position_ids&quot;</span><span class="p">:</span> <span class="n">position_ids</span><span class="p">,</span>
    <span class="p">}</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="src.python.easydel.modules.mixtral.modelling_mixtral_flax.FlaxMixtralSparseMoeBlock" class="doc doc-heading">
            <code>FlaxMixtralSparseMoeBlock</code>


</h2>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="flax.linen.Module">Module</span></code></p>


      <p>This implementation is
strictly equivalent to standard MoE with full capacity (no
dropped tokens). It's faster since it formulates MoE operations
in terms of block-sparse operations to accomodate imbalanced
assignments of tokens to experts, whereas standard MoE either
(1) drop tokens at the cost of reduced performance or (2) set
capacity factor to number of experts and thus waste computation
and memory on padding.</p>

              <details class="quote">
                <summary>Source code in <code>src/python/easydel/modules/mixtral/modelling_mixtral_flax.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">FlaxMixtralSparseMoeBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;This implementation is</span>
<span class="sd">    strictly equivalent to standard MoE with full capacity (no</span>
<span class="sd">    dropped tokens). It&#39;s faster since it formulates MoE operations</span>
<span class="sd">    in terms of block-sparse operations to accomodate imbalanced</span>
<span class="sd">    assignments of tokens to experts, whereas standard MoE either</span>
<span class="sd">    (1) drop tokens at the cost of reduced performance or (2) set</span>
<span class="sd">    capacity factor to number of experts and thus waste computation</span>
<span class="sd">    and memory on padding.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">config</span><span class="p">:</span> <span class="n">MixtralConfig</span>
    <span class="n">dtype</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">bfloat16</span>
    <span class="n">param_dtype</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">bfloat16</span>
    <span class="n">precision</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
        <span class="n">Union</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">Precision</span><span class="p">]</span>
    <span class="p">]</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">Precision</span><span class="p">(</span><span class="s2">&quot;fastest&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">setup</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gate</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_local_experts</span><span class="p">,</span>
            <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">param_dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">param_dtype</span><span class="p">,</span>
            <span class="n">precision</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">precision</span><span class="p">,</span>
            <span class="n">kernel_init</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">initializers</span><span class="o">.</span><span class="n">normal</span><span class="p">(),</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">experts</span> <span class="o">=</span> <span class="n">FlaxMixtralBlocKSparesTop2MLPCollection</span><span class="p">(</span>
            <span class="n">config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">param_dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">param_dtype</span><span class="p">,</span>
            <span class="n">precision</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">precision</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">hidden_states</span><span class="p">:</span> <span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span>
            <span class="n">e</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>  <span class="c1"># Ignored</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span> <span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">]:</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span>

        <span class="n">router_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gate</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span>  <span class="c1"># no reshaping is needed</span>
            <span class="n">jnp</span><span class="o">.</span><span class="n">promote_types</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">routing_weights</span><span class="p">,</span> <span class="n">selected_experts</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">top_k</span><span class="p">(</span>
            <span class="n">router_logits</span><span class="p">,</span>
            <span class="n">k</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_experts_per_tok</span>
        <span class="p">)</span>
        <span class="n">routing_weights</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span>
            <span class="n">routing_weights</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span>
                <span class="n">jnp</span><span class="o">.</span><span class="n">promote_types</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
            <span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">experts</span><span class="p">(</span>
            <span class="n">selected_experts</span><span class="o">=</span><span class="n">selected_experts</span><span class="p">,</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
            <span class="n">sequence_length</span><span class="o">=</span><span class="n">sequence_length</span><span class="p">,</span>
            <span class="n">hidden_dim</span><span class="o">=</span><span class="n">hidden_dim</span><span class="p">,</span>
            <span class="n">hidden_states</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">,</span>
            <span class="n">routing_weights</span><span class="o">=</span><span class="n">routing_weights</span>
        <span class="p">),</span> <span class="n">router_logits</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="src.python.easydel.modules.mixtral.modelling_mixtral_flax.MixtralPreTrainedModel" class="doc doc-heading">
            <code>MixtralPreTrainedModel</code>


</h2>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><a class="autorefs autorefs-internal" title="src.python.easydel.modules.easydel_modelling_utils.EasyDeLFlaxPretrainedModel" href="../generated-modules-easydel_modelling_utils/#src.python.easydel.modules.easydel_modelling_utils.EasyDeLFlaxPretrainedModel">EasyDeLFlaxPretrainedModel</a></code></p>


              <details class="quote">
                <summary>Source code in <code>src/python/easydel/modules/mixtral/modelling_mixtral_flax.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">697</span>
<span class="normal">698</span>
<span class="normal">699</span>
<span class="normal">700</span>
<span class="normal">701</span>
<span class="normal">702</span>
<span class="normal">703</span>
<span class="normal">704</span>
<span class="normal">705</span>
<span class="normal">706</span>
<span class="normal">707</span>
<span class="normal">708</span>
<span class="normal">709</span>
<span class="normal">710</span>
<span class="normal">711</span>
<span class="normal">712</span>
<span class="normal">713</span>
<span class="normal">714</span>
<span class="normal">715</span>
<span class="normal">716</span>
<span class="normal">717</span>
<span class="normal">718</span>
<span class="normal">719</span>
<span class="normal">720</span>
<span class="normal">721</span>
<span class="normal">722</span>
<span class="normal">723</span>
<span class="normal">724</span>
<span class="normal">725</span>
<span class="normal">726</span>
<span class="normal">727</span>
<span class="normal">728</span>
<span class="normal">729</span>
<span class="normal">730</span>
<span class="normal">731</span>
<span class="normal">732</span>
<span class="normal">733</span>
<span class="normal">734</span>
<span class="normal">735</span>
<span class="normal">736</span>
<span class="normal">737</span>
<span class="normal">738</span>
<span class="normal">739</span>
<span class="normal">740</span>
<span class="normal">741</span>
<span class="normal">742</span>
<span class="normal">743</span>
<span class="normal">744</span>
<span class="normal">745</span>
<span class="normal">746</span>
<span class="normal">747</span>
<span class="normal">748</span>
<span class="normal">749</span>
<span class="normal">750</span>
<span class="normal">751</span>
<span class="normal">752</span>
<span class="normal">753</span>
<span class="normal">754</span>
<span class="normal">755</span>
<span class="normal">756</span>
<span class="normal">757</span>
<span class="normal">758</span>
<span class="normal">759</span>
<span class="normal">760</span>
<span class="normal">761</span>
<span class="normal">762</span>
<span class="normal">763</span>
<span class="normal">764</span>
<span class="normal">765</span>
<span class="normal">766</span>
<span class="normal">767</span>
<span class="normal">768</span>
<span class="normal">769</span>
<span class="normal">770</span>
<span class="normal">771</span>
<span class="normal">772</span>
<span class="normal">773</span>
<span class="normal">774</span>
<span class="normal">775</span>
<span class="normal">776</span>
<span class="normal">777</span>
<span class="normal">778</span>
<span class="normal">779</span>
<span class="normal">780</span>
<span class="normal">781</span>
<span class="normal">782</span>
<span class="normal">783</span>
<span class="normal">784</span>
<span class="normal">785</span>
<span class="normal">786</span>
<span class="normal">787</span>
<span class="normal">788</span>
<span class="normal">789</span>
<span class="normal">790</span>
<span class="normal">791</span>
<span class="normal">792</span>
<span class="normal">793</span>
<span class="normal">794</span>
<span class="normal">795</span>
<span class="normal">796</span>
<span class="normal">797</span>
<span class="normal">798</span>
<span class="normal">799</span>
<span class="normal">800</span>
<span class="normal">801</span>
<span class="normal">802</span>
<span class="normal">803</span>
<span class="normal">804</span>
<span class="normal">805</span>
<span class="normal">806</span>
<span class="normal">807</span>
<span class="normal">808</span>
<span class="normal">809</span>
<span class="normal">810</span>
<span class="normal">811</span>
<span class="normal">812</span>
<span class="normal">813</span>
<span class="normal">814</span>
<span class="normal">815</span>
<span class="normal">816</span>
<span class="normal">817</span>
<span class="normal">818</span>
<span class="normal">819</span>
<span class="normal">820</span>
<span class="normal">821</span>
<span class="normal">822</span>
<span class="normal">823</span>
<span class="normal">824</span>
<span class="normal">825</span>
<span class="normal">826</span>
<span class="normal">827</span>
<span class="normal">828</span>
<span class="normal">829</span>
<span class="normal">830</span>
<span class="normal">831</span>
<span class="normal">832</span>
<span class="normal">833</span>
<span class="normal">834</span>
<span class="normal">835</span>
<span class="normal">836</span>
<span class="normal">837</span>
<span class="normal">838</span>
<span class="normal">839</span>
<span class="normal">840</span>
<span class="normal">841</span>
<span class="normal">842</span>
<span class="normal">843</span>
<span class="normal">844</span>
<span class="normal">845</span>
<span class="normal">846</span>
<span class="normal">847</span>
<span class="normal">848</span>
<span class="normal">849</span>
<span class="normal">850</span>
<span class="normal">851</span>
<span class="normal">852</span>
<span class="normal">853</span>
<span class="normal">854</span>
<span class="normal">855</span>
<span class="normal">856</span>
<span class="normal">857</span>
<span class="normal">858</span>
<span class="normal">859</span>
<span class="normal">860</span>
<span class="normal">861</span>
<span class="normal">862</span>
<span class="normal">863</span>
<span class="normal">864</span>
<span class="normal">865</span>
<span class="normal">866</span>
<span class="normal">867</span>
<span class="normal">868</span>
<span class="normal">869</span>
<span class="normal">870</span>
<span class="normal">871</span>
<span class="normal">872</span>
<span class="normal">873</span>
<span class="normal">874</span>
<span class="normal">875</span>
<span class="normal">876</span>
<span class="normal">877</span>
<span class="normal">878</span>
<span class="normal">879</span>
<span class="normal">880</span>
<span class="normal">881</span>
<span class="normal">882</span>
<span class="normal">883</span>
<span class="normal">884</span>
<span class="normal">885</span>
<span class="normal">886</span>
<span class="normal">887</span>
<span class="normal">888</span>
<span class="normal">889</span>
<span class="normal">890</span>
<span class="normal">891</span>
<span class="normal">892</span>
<span class="normal">893</span>
<span class="normal">894</span>
<span class="normal">895</span>
<span class="normal">896</span>
<span class="normal">897</span>
<span class="normal">898</span>
<span class="normal">899</span>
<span class="normal">900</span>
<span class="normal">901</span>
<span class="normal">902</span>
<span class="normal">903</span>
<span class="normal">904</span>
<span class="normal">905</span>
<span class="normal">906</span>
<span class="normal">907</span>
<span class="normal">908</span>
<span class="normal">909</span>
<span class="normal">910</span>
<span class="normal">911</span>
<span class="normal">912</span>
<span class="normal">913</span>
<span class="normal">914</span>
<span class="normal">915</span>
<span class="normal">916</span>
<span class="normal">917</span>
<span class="normal">918</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">MixtralPreTrainedModel</span><span class="p">(</span><span class="n">EasyDeLFlaxPretrainedModel</span><span class="p">):</span>
    <span class="n">config_class</span><span class="p">:</span> <span class="n">MixtralConfig</span> <span class="o">=</span> <span class="n">MixtralConfig</span>
    <span class="n">module_class</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">base_model_prefix</span> <span class="o">=</span> <span class="s2">&quot;model&quot;</span>

    <span class="c1"># main_input_name = &quot;input_ids&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">config</span><span class="p">:</span> <span class="n">MixtralConfig</span><span class="p">,</span>
            <span class="n">dtype</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>
            <span class="n">param_dtype</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>
            <span class="n">precision</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">Precision</span><span class="p">]</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">Precision</span><span class="p">(</span>
                <span class="s2">&quot;fastest&quot;</span><span class="p">),</span>
            <span class="n">input_shape</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
            <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
            <span class="n">_do_init</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">):</span>
        <span class="n">module</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">module_class</span><span class="p">(</span>
            <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">param_dtype</span><span class="o">=</span><span class="n">param_dtype</span><span class="p">,</span>
            <span class="n">precision</span><span class="o">=</span><span class="n">precision</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span>
        <span class="p">)</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">_do_init</span><span class="o">=</span><span class="n">_do_init</span><span class="p">,</span>
            <span class="n">module</span><span class="o">=</span><span class="n">module</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="n">input_shape</span><span class="p">,</span>
            <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">init_weights</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">rng</span><span class="p">:</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">,</span>
            <span class="n">input_shape</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">,</span>
            <span class="n">params</span><span class="p">:</span> <span class="n">FrozenDict</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">FrozenDict</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;The init_weights function is used to initialize the weights of a model.</span>
<span class="sd">        It takes in a rng, which is a random number generator key that can be used to generate random numbers.</span>
<span class="sd">        The input_shape parameter specifies the shape of the inputs that will be fed into this model.</span>
<span class="sd">        The params parameter allows you to pass in pre-trained weights for your model, if you have them available.</span>

<span class="sd">        Args:</span>
<span class="sd">            self: Access variables that belong to the class</span>
<span class="sd">            rng: jax.random.PRNGKey: Initialize the weights of the model</span>
<span class="sd">            input_shape: Tuple: Initialize the input_ids, attention_mask</span>
<span class="sd">                and position_ids</span>
<span class="sd">            params: flax.core.FrozenDict: Pass in the parameters of a</span>
<span class="sd">                pre-trained model</span>

<span class="sd">        Returns:</span>
<span class="sd">            A frozendict of parameters</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">initialization_of_moe</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">input_ids</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;i4&quot;</span><span class="p">)</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;i4&quot;</span><span class="p">)</span>
        <span class="n">position_ids</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span>
            <span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;i4&quot;</span><span class="p">),</span>
            <span class="n">input_shape</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">params_rng</span><span class="p">,</span> <span class="n">dropout_rng</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">rng</span><span class="p">)</span>
        <span class="n">rngs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="n">params_rng</span><span class="p">,</span> <span class="s2">&quot;dropout&quot;</span><span class="p">:</span> <span class="n">dropout_rng</span><span class="p">}</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">add_cross_attention</span><span class="p">:</span>
            <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
                <span class="n">input_shape</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,))</span>
            <span class="n">encoder_attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span>
            <span class="n">module_init_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">init</span><span class="p">(</span>
                <span class="n">rngs</span><span class="p">,</span>
                <span class="n">input_ids</span><span class="p">,</span>
                <span class="n">attention_mask</span><span class="p">,</span>
                <span class="n">position_ids</span><span class="p">,</span>
                <span class="n">encoder_hidden_states</span><span class="p">,</span>
                <span class="n">encoder_attention_mask</span><span class="p">,</span>
                <span class="n">return_dict</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">module_init_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">init</span><span class="p">(</span>
                <span class="n">rngs</span><span class="p">,</span>
                <span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span>
                <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
                <span class="n">position_ids</span><span class="o">=</span><span class="n">position_ids</span><span class="p">,</span>
                <span class="n">return_dict</span><span class="o">=</span><span class="kc">False</span>
            <span class="p">)</span>
        <span class="n">random_params</span> <span class="o">=</span> <span class="n">module_init_outputs</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">initialization_of_moe</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="n">params</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">random_params</span> <span class="o">=</span> <span class="n">flatten_dict</span><span class="p">(</span><span class="n">unfreeze</span><span class="p">(</span><span class="n">random_params</span><span class="p">))</span>
            <span class="n">params</span> <span class="o">=</span> <span class="n">flatten_dict</span><span class="p">(</span><span class="n">unfreeze</span><span class="p">(</span><span class="n">params</span><span class="p">))</span>
            <span class="k">for</span> <span class="n">missing_key</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_missing_keys</span><span class="p">:</span>
                <span class="n">params</span><span class="p">[</span><span class="n">missing_key</span><span class="p">]</span> <span class="o">=</span> <span class="n">random_params</span><span class="p">[</span><span class="n">missing_key</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_missing_keys</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
            <span class="k">return</span> <span class="n">freeze</span><span class="p">(</span><span class="n">unflatten_dict</span><span class="p">(</span><span class="n">params</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">random_params</span>

    <span class="k">def</span> <span class="nf">init_cache</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">max_length</span><span class="p">):</span>

        <span class="n">input_ids</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">max_length</span><span class="p">))</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>
        <span class="n">position_ids</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span>
            <span class="n">jnp</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

        <span class="n">init_variables</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">init</span><span class="p">(</span>
            <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">position_ids</span><span class="p">,</span> <span class="n">return_dict</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">init_cache</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">init_variables</span><span class="p">[</span><span class="s2">&quot;cache&quot;</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">input_ids</span><span class="p">:</span> <span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">position_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">params</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">past_key_values</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">dropout_rng</span><span class="p">:</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">train</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
            <span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">output_router_logits</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">return_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">add_params_field</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;The __call__ function is the main function of a JAX module.</span>
<span class="sd">        It takes as input:</span>
<span class="sd">        - The parameters of the model (self.params)</span>
<span class="sd">        - The inputs to the model (input_ids, attention_mask, position_ids)</span>
<span class="sd">        - Whether we are training (train=True/False) and whether we want to return all hidden states and</span>
<span class="sd">        attentions weights at each layer in addition to just the last layer output (output_hidden_states=True/False).</span>

<span class="sd">        Args:</span>
<span class="sd">            self: Represent the instance of the class</span>
<span class="sd">            input_ids: Pass the input sequence to the model</span>
<span class="sd">            attention_mask: Mask out the padding tokens</span>
<span class="sd">            position_ids: Specify the position of each token in the</span>
<span class="sd">                sequence</span>
<span class="sd">            params: dict: Pass in the parameters of the model</span>
<span class="sd">            past_key_values: dict: Pass the past key values to the model</span>
<span class="sd">            dropout_rng: jax.random.PRNGKey: Pass in a random number</span>
<span class="sd">                generator key to the model</span>
<span class="sd">            train: bool: Determine whether to use dropout or not</span>
<span class="sd">            output_attentions: Optional[bool]: Determine whether to</span>
<span class="sd">                return the attention weights</span>
<span class="sd">            output_hidden_states: Optional[bool]: Determine whether to</span>
<span class="sd">                return the hidden states of all layers</span>
<span class="sd">            return_dict: Optional[bool]: Return a dictionary of the</span>
<span class="sd">                outputs</span>
<span class="sd">            add_params_field: bool: Add a params field to the inputs</span>
<span class="sd">                dictionary</span>

<span class="sd">        Returns:</span>
<span class="sd">            A tuple of (last_hidden_state, past_key_values)</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">output_attentions</span> <span class="o">=</span> <span class="n">output_attentions</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attentions</span>
        <span class="n">output_hidden_states</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">output_hidden_states</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_hidden_states</span>
        <span class="p">)</span>
        <span class="n">return_dict</span> <span class="o">=</span> <span class="n">return_dict</span> <span class="k">if</span> <span class="n">return_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">return_dict</span>

        <span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span>

        <span class="k">if</span> <span class="n">position_ids</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">past_key_values</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;Make sure to provide `position_ids` when passing `past_key_values`.&quot;</span><span class="p">)</span>

            <span class="n">position_ids</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">sequence_length</span><span class="p">)[</span>
                                            <span class="kc">None</span><span class="p">,</span> <span class="p">:],</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">))</span>

        <span class="n">rng_s</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">if</span> <span class="n">dropout_rng</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">rng_s</span><span class="p">[</span><span class="s2">&quot;dropout&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">dropout_rng</span>

        <span class="n">inputs</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="n">params</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">}</span> <span class="k">if</span> <span class="n">add_params_field</span> <span class="k">else</span> <span class="n">params</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">bits</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">rng_s</span><span class="p">[</span><span class="s1">&#39;params&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">key</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">past_key_values</span><span class="p">:</span>
            <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;cache&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">past_key_values</span>
            <span class="n">mutable</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;cache&quot;</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">mutable</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;i4&quot;</span><span class="p">),</span>  <span class="c1"># input_ids: chex.Array</span>
            <span class="c1"># attention_mask: Optional[chex.Array] = None</span>
            <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;i4&quot;</span><span class="p">),</span>
            <span class="c1"># position_ids: Optional[chex.Array] = None</span>
            <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">position_ids</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;i4&quot;</span><span class="p">),</span>
            <span class="kc">None</span><span class="p">,</span>  <span class="c1"># inputs_embeds: Optional[chex.Array] = None</span>
            <span class="n">output_attentions</span><span class="p">,</span>  <span class="c1"># output_attentions: Optional[bool] = None</span>
            <span class="c1"># output_hidden_states: Optional[bool] = None</span>
            <span class="n">output_hidden_states</span><span class="p">,</span>
            <span class="c1"># output_router_logits: Optional[bool] = None</span>
            <span class="n">output_router_logits</span><span class="p">,</span>
            <span class="kc">False</span><span class="p">,</span>  <span class="c1"># init_cache: bool = False</span>
            <span class="ow">not</span> <span class="n">train</span><span class="p">,</span>  <span class="c1"># deterministic: bool = True</span>
            <span class="n">return_dict</span><span class="p">,</span>  <span class="c1"># return_dict: bool = True</span>
            <span class="n">rngs</span><span class="o">=</span><span class="n">rng_s</span><span class="p">,</span>
            <span class="n">mutable</span><span class="o">=</span><span class="n">mutable</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">past_key_values</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">return_dict</span><span class="p">:</span>
            <span class="n">outputs</span><span class="p">,</span> <span class="n">past_key_values</span> <span class="o">=</span> <span class="n">outputs</span>
            <span class="n">outputs</span><span class="p">[</span><span class="s2">&quot;past_key_values&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">unfreeze</span><span class="p">(</span><span class="n">past_key_values</span><span class="p">[</span><span class="s2">&quot;cache&quot;</span><span class="p">])</span>
            <span class="k">return</span> <span class="n">outputs</span>
        <span class="k">elif</span> <span class="n">past_key_values</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
            <span class="n">outputs</span><span class="p">,</span> <span class="n">past_key_values</span> <span class="o">=</span> <span class="n">outputs</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[:</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> \
                      <span class="p">(</span><span class="n">unfreeze</span><span class="p">(</span><span class="n">past_key_values</span><span class="p">[</span><span class="s2">&quot;cache&quot;</span><span class="p">]),)</span> <span class="o">+</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>

        <span class="k">return</span> <span class="n">outputs</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="src.python.easydel.modules.mixtral.modelling_mixtral_flax.MixtralPreTrainedModel.__call__" class="doc doc-heading">
            <code class="highlight language-python"><span class="fm">__call__</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">position_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">past_key_values</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dropout_rng</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_hidden_states</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_router_logits</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">return_dict</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">add_params_field</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

      <p>The <strong>call</strong> function is the main function of a JAX module.
It takes as input:
- The parameters of the model (self.params)
- The inputs to the model (input_ids, attention_mask, position_ids)
- Whether we are training (train=True/False) and whether we want to return all hidden states and
attentions weights at each layer in addition to just the last layer output (output_hidden_states=True/False).</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>self</code></td>
            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Represent the instance of the class</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>input_ids</code></td>
            <td>
                  <code><span title="chex.Array">Array</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Pass the input sequence to the model</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>attention_mask</code></td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="chex.Array">Array</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Mask out the padding tokens</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>position_ids</code></td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="chex.Array">Array</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Specify the position of each token in the
sequence</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>params</code></td>
            <td>
                  <code>dict</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>dict: Pass in the parameters of the model</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>past_key_values</code></td>
            <td>
                  <code>dict</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>dict: Pass the past key values to the model</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>dropout_rng</code></td>
            <td>
                  <code><span title="jax.random.PRNGKey">PRNGKey</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>jax.random.PRNGKey: Pass in a random number
generator key to the model</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>train</code></td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>bool: Determine whether to use dropout or not</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>output_attentions</code></td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[bool]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Optional[bool]: Determine whether to
return the attention weights</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>output_hidden_states</code></td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[bool]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Optional[bool]: Determine whether to
return the hidden states of all layers</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>return_dict</code></td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[bool]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Optional[bool]: Return a dictionary of the
outputs</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>add_params_field</code></td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>bool: Add a params field to the inputs
dictionary</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>A tuple of (last_hidden_state, past_key_values)</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>src/python/easydel/modules/mixtral/modelling_mixtral_flax.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">808</span>
<span class="normal">809</span>
<span class="normal">810</span>
<span class="normal">811</span>
<span class="normal">812</span>
<span class="normal">813</span>
<span class="normal">814</span>
<span class="normal">815</span>
<span class="normal">816</span>
<span class="normal">817</span>
<span class="normal">818</span>
<span class="normal">819</span>
<span class="normal">820</span>
<span class="normal">821</span>
<span class="normal">822</span>
<span class="normal">823</span>
<span class="normal">824</span>
<span class="normal">825</span>
<span class="normal">826</span>
<span class="normal">827</span>
<span class="normal">828</span>
<span class="normal">829</span>
<span class="normal">830</span>
<span class="normal">831</span>
<span class="normal">832</span>
<span class="normal">833</span>
<span class="normal">834</span>
<span class="normal">835</span>
<span class="normal">836</span>
<span class="normal">837</span>
<span class="normal">838</span>
<span class="normal">839</span>
<span class="normal">840</span>
<span class="normal">841</span>
<span class="normal">842</span>
<span class="normal">843</span>
<span class="normal">844</span>
<span class="normal">845</span>
<span class="normal">846</span>
<span class="normal">847</span>
<span class="normal">848</span>
<span class="normal">849</span>
<span class="normal">850</span>
<span class="normal">851</span>
<span class="normal">852</span>
<span class="normal">853</span>
<span class="normal">854</span>
<span class="normal">855</span>
<span class="normal">856</span>
<span class="normal">857</span>
<span class="normal">858</span>
<span class="normal">859</span>
<span class="normal">860</span>
<span class="normal">861</span>
<span class="normal">862</span>
<span class="normal">863</span>
<span class="normal">864</span>
<span class="normal">865</span>
<span class="normal">866</span>
<span class="normal">867</span>
<span class="normal">868</span>
<span class="normal">869</span>
<span class="normal">870</span>
<span class="normal">871</span>
<span class="normal">872</span>
<span class="normal">873</span>
<span class="normal">874</span>
<span class="normal">875</span>
<span class="normal">876</span>
<span class="normal">877</span>
<span class="normal">878</span>
<span class="normal">879</span>
<span class="normal">880</span>
<span class="normal">881</span>
<span class="normal">882</span>
<span class="normal">883</span>
<span class="normal">884</span>
<span class="normal">885</span>
<span class="normal">886</span>
<span class="normal">887</span>
<span class="normal">888</span>
<span class="normal">889</span>
<span class="normal">890</span>
<span class="normal">891</span>
<span class="normal">892</span>
<span class="normal">893</span>
<span class="normal">894</span>
<span class="normal">895</span>
<span class="normal">896</span>
<span class="normal">897</span>
<span class="normal">898</span>
<span class="normal">899</span>
<span class="normal">900</span>
<span class="normal">901</span>
<span class="normal">902</span>
<span class="normal">903</span>
<span class="normal">904</span>
<span class="normal">905</span>
<span class="normal">906</span>
<span class="normal">907</span>
<span class="normal">908</span>
<span class="normal">909</span>
<span class="normal">910</span>
<span class="normal">911</span>
<span class="normal">912</span>
<span class="normal">913</span>
<span class="normal">914</span>
<span class="normal">915</span>
<span class="normal">916</span>
<span class="normal">917</span>
<span class="normal">918</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="p">:</span> <span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">position_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">chex</span><span class="o">.</span><span class="n">Array</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">params</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">past_key_values</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">dropout_rng</span><span class="p">:</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">train</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">output_attentions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_hidden_states</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_router_logits</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">add_params_field</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;The __call__ function is the main function of a JAX module.</span>
<span class="sd">    It takes as input:</span>
<span class="sd">    - The parameters of the model (self.params)</span>
<span class="sd">    - The inputs to the model (input_ids, attention_mask, position_ids)</span>
<span class="sd">    - Whether we are training (train=True/False) and whether we want to return all hidden states and</span>
<span class="sd">    attentions weights at each layer in addition to just the last layer output (output_hidden_states=True/False).</span>

<span class="sd">    Args:</span>
<span class="sd">        self: Represent the instance of the class</span>
<span class="sd">        input_ids: Pass the input sequence to the model</span>
<span class="sd">        attention_mask: Mask out the padding tokens</span>
<span class="sd">        position_ids: Specify the position of each token in the</span>
<span class="sd">            sequence</span>
<span class="sd">        params: dict: Pass in the parameters of the model</span>
<span class="sd">        past_key_values: dict: Pass the past key values to the model</span>
<span class="sd">        dropout_rng: jax.random.PRNGKey: Pass in a random number</span>
<span class="sd">            generator key to the model</span>
<span class="sd">        train: bool: Determine whether to use dropout or not</span>
<span class="sd">        output_attentions: Optional[bool]: Determine whether to</span>
<span class="sd">            return the attention weights</span>
<span class="sd">        output_hidden_states: Optional[bool]: Determine whether to</span>
<span class="sd">            return the hidden states of all layers</span>
<span class="sd">        return_dict: Optional[bool]: Return a dictionary of the</span>
<span class="sd">            outputs</span>
<span class="sd">        add_params_field: bool: Add a params field to the inputs</span>
<span class="sd">            dictionary</span>

<span class="sd">    Returns:</span>
<span class="sd">        A tuple of (last_hidden_state, past_key_values)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">output_attentions</span> <span class="o">=</span> <span class="n">output_attentions</span> <span class="k">if</span> <span class="n">output_attentions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attentions</span>
    <span class="n">output_hidden_states</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">output_hidden_states</span> <span class="k">if</span> <span class="n">output_hidden_states</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_hidden_states</span>
    <span class="p">)</span>
    <span class="n">return_dict</span> <span class="o">=</span> <span class="n">return_dict</span> <span class="k">if</span> <span class="n">return_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">return_dict</span>

    <span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span>

    <span class="k">if</span> <span class="n">position_ids</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">past_key_values</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Make sure to provide `position_ids` when passing `past_key_values`.&quot;</span><span class="p">)</span>

        <span class="n">position_ids</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">sequence_length</span><span class="p">)[</span>
                                        <span class="kc">None</span><span class="p">,</span> <span class="p">:],</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">))</span>

    <span class="n">rng_s</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">if</span> <span class="n">dropout_rng</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">rng_s</span><span class="p">[</span><span class="s2">&quot;dropout&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">dropout_rng</span>

    <span class="n">inputs</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="n">params</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">}</span> <span class="k">if</span> <span class="n">add_params_field</span> <span class="k">else</span> <span class="n">params</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">bits</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">rng_s</span><span class="p">[</span><span class="s1">&#39;params&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">key</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">past_key_values</span><span class="p">:</span>
        <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;cache&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">past_key_values</span>
        <span class="n">mutable</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;cache&quot;</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">mutable</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span>
        <span class="n">inputs</span><span class="p">,</span>
        <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;i4&quot;</span><span class="p">),</span>  <span class="c1"># input_ids: chex.Array</span>
        <span class="c1"># attention_mask: Optional[chex.Array] = None</span>
        <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;i4&quot;</span><span class="p">),</span>
        <span class="c1"># position_ids: Optional[chex.Array] = None</span>
        <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">position_ids</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;i4&quot;</span><span class="p">),</span>
        <span class="kc">None</span><span class="p">,</span>  <span class="c1"># inputs_embeds: Optional[chex.Array] = None</span>
        <span class="n">output_attentions</span><span class="p">,</span>  <span class="c1"># output_attentions: Optional[bool] = None</span>
        <span class="c1"># output_hidden_states: Optional[bool] = None</span>
        <span class="n">output_hidden_states</span><span class="p">,</span>
        <span class="c1"># output_router_logits: Optional[bool] = None</span>
        <span class="n">output_router_logits</span><span class="p">,</span>
        <span class="kc">False</span><span class="p">,</span>  <span class="c1"># init_cache: bool = False</span>
        <span class="ow">not</span> <span class="n">train</span><span class="p">,</span>  <span class="c1"># deterministic: bool = True</span>
        <span class="n">return_dict</span><span class="p">,</span>  <span class="c1"># return_dict: bool = True</span>
        <span class="n">rngs</span><span class="o">=</span><span class="n">rng_s</span><span class="p">,</span>
        <span class="n">mutable</span><span class="o">=</span><span class="n">mutable</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">past_key_values</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">return_dict</span><span class="p">:</span>
        <span class="n">outputs</span><span class="p">,</span> <span class="n">past_key_values</span> <span class="o">=</span> <span class="n">outputs</span>
        <span class="n">outputs</span><span class="p">[</span><span class="s2">&quot;past_key_values&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">unfreeze</span><span class="p">(</span><span class="n">past_key_values</span><span class="p">[</span><span class="s2">&quot;cache&quot;</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">outputs</span>
    <span class="k">elif</span> <span class="n">past_key_values</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">return_dict</span><span class="p">:</span>
        <span class="n">outputs</span><span class="p">,</span> <span class="n">past_key_values</span> <span class="o">=</span> <span class="n">outputs</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[:</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> \
                  <span class="p">(</span><span class="n">unfreeze</span><span class="p">(</span><span class="n">past_key_values</span><span class="p">[</span><span class="s2">&quot;cache&quot;</span><span class="p">]),)</span> <span class="o">+</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>

    <span class="k">return</span> <span class="n">outputs</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="src.python.easydel.modules.mixtral.modelling_mixtral_flax.MixtralPreTrainedModel.init_weights" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">init_weights</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

</h3>


    <div class="doc doc-contents ">

      <p>The init_weights function is used to initialize the weights of a model.
It takes in a rng, which is a random number generator key that can be used to generate random numbers.
The input_shape parameter specifies the shape of the inputs that will be fed into this model.
The params parameter allows you to pass in pre-trained weights for your model, if you have them available.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code>self</code></td>
            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Access variables that belong to the class</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>rng</code></td>
            <td>
                  <code><span title="jax.random.PRNGKey">PRNGKey</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>jax.random.PRNGKey: Initialize the weights of the model</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>input_shape</code></td>
            <td>
                  <code><span title="typing.Tuple">Tuple</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Tuple: Initialize the input_ids, attention_mask
and position_ids</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code>params</code></td>
            <td>
                  <code><span title="flax.core.FrozenDict">FrozenDict</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>flax.core.FrozenDict: Pass in the parameters of a
pre-trained model</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                  <code><span title="flax.core.FrozenDict">FrozenDict</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>A frozendict of parameters</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>src/python/easydel/modules/mixtral/modelling_mixtral_flax.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">730</span>
<span class="normal">731</span>
<span class="normal">732</span>
<span class="normal">733</span>
<span class="normal">734</span>
<span class="normal">735</span>
<span class="normal">736</span>
<span class="normal">737</span>
<span class="normal">738</span>
<span class="normal">739</span>
<span class="normal">740</span>
<span class="normal">741</span>
<span class="normal">742</span>
<span class="normal">743</span>
<span class="normal">744</span>
<span class="normal">745</span>
<span class="normal">746</span>
<span class="normal">747</span>
<span class="normal">748</span>
<span class="normal">749</span>
<span class="normal">750</span>
<span class="normal">751</span>
<span class="normal">752</span>
<span class="normal">753</span>
<span class="normal">754</span>
<span class="normal">755</span>
<span class="normal">756</span>
<span class="normal">757</span>
<span class="normal">758</span>
<span class="normal">759</span>
<span class="normal">760</span>
<span class="normal">761</span>
<span class="normal">762</span>
<span class="normal">763</span>
<span class="normal">764</span>
<span class="normal">765</span>
<span class="normal">766</span>
<span class="normal">767</span>
<span class="normal">768</span>
<span class="normal">769</span>
<span class="normal">770</span>
<span class="normal">771</span>
<span class="normal">772</span>
<span class="normal">773</span>
<span class="normal">774</span>
<span class="normal">775</span>
<span class="normal">776</span>
<span class="normal">777</span>
<span class="normal">778</span>
<span class="normal">779</span>
<span class="normal">780</span>
<span class="normal">781</span>
<span class="normal">782</span>
<span class="normal">783</span>
<span class="normal">784</span>
<span class="normal">785</span>
<span class="normal">786</span>
<span class="normal">787</span>
<span class="normal">788</span>
<span class="normal">789</span>
<span class="normal">790</span>
<span class="normal">791</span>
<span class="normal">792</span>
<span class="normal">793</span>
<span class="normal">794</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">init_weights</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">rng</span><span class="p">:</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">,</span>
        <span class="n">input_shape</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">,</span>
        <span class="n">params</span><span class="p">:</span> <span class="n">FrozenDict</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">FrozenDict</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;The init_weights function is used to initialize the weights of a model.</span>
<span class="sd">    It takes in a rng, which is a random number generator key that can be used to generate random numbers.</span>
<span class="sd">    The input_shape parameter specifies the shape of the inputs that will be fed into this model.</span>
<span class="sd">    The params parameter allows you to pass in pre-trained weights for your model, if you have them available.</span>

<span class="sd">    Args:</span>
<span class="sd">        self: Access variables that belong to the class</span>
<span class="sd">        rng: jax.random.PRNGKey: Initialize the weights of the model</span>
<span class="sd">        input_shape: Tuple: Initialize the input_ids, attention_mask</span>
<span class="sd">            and position_ids</span>
<span class="sd">        params: flax.core.FrozenDict: Pass in the parameters of a</span>
<span class="sd">            pre-trained model</span>

<span class="sd">    Returns:</span>
<span class="sd">        A frozendict of parameters</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">initialization_of_moe</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">input_ids</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;i4&quot;</span><span class="p">)</span>
    <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;i4&quot;</span><span class="p">)</span>
    <span class="n">position_ids</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span>
        <span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;i4&quot;</span><span class="p">),</span>
        <span class="n">input_shape</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">params_rng</span><span class="p">,</span> <span class="n">dropout_rng</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">rng</span><span class="p">)</span>
    <span class="n">rngs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="n">params_rng</span><span class="p">,</span> <span class="s2">&quot;dropout&quot;</span><span class="p">:</span> <span class="n">dropout_rng</span><span class="p">}</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">add_cross_attention</span><span class="p">:</span>
        <span class="n">encoder_hidden_states</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
            <span class="n">input_shape</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,))</span>
        <span class="n">encoder_attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span>
        <span class="n">module_init_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">init</span><span class="p">(</span>
            <span class="n">rngs</span><span class="p">,</span>
            <span class="n">input_ids</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="p">,</span>
            <span class="n">position_ids</span><span class="p">,</span>
            <span class="n">encoder_hidden_states</span><span class="p">,</span>
            <span class="n">encoder_attention_mask</span><span class="p">,</span>
            <span class="n">return_dict</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">module_init_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">init</span><span class="p">(</span>
            <span class="n">rngs</span><span class="p">,</span>
            <span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
            <span class="n">position_ids</span><span class="o">=</span><span class="n">position_ids</span><span class="p">,</span>
            <span class="n">return_dict</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>
    <span class="n">random_params</span> <span class="o">=</span> <span class="n">module_init_outputs</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">initialization_of_moe</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="n">params</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">random_params</span> <span class="o">=</span> <span class="n">flatten_dict</span><span class="p">(</span><span class="n">unfreeze</span><span class="p">(</span><span class="n">random_params</span><span class="p">))</span>
        <span class="n">params</span> <span class="o">=</span> <span class="n">flatten_dict</span><span class="p">(</span><span class="n">unfreeze</span><span class="p">(</span><span class="n">params</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">missing_key</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_missing_keys</span><span class="p">:</span>
            <span class="n">params</span><span class="p">[</span><span class="n">missing_key</span><span class="p">]</span> <span class="o">=</span> <span class="n">random_params</span><span class="p">[</span><span class="n">missing_key</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_missing_keys</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">freeze</span><span class="p">(</span><span class="n">unflatten_dict</span><span class="p">(</span><span class="n">params</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">random_params</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Erfan Zare Chavoshi-easydel
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "..", "features": [], "search": "../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.081f42fc.min.js"></script>
      
    
  </body>
</html>