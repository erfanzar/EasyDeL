# Copyright 2025 The EasyDeL Author @erfanzar (Erfan Zare Chavoshi).
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Vanilla (standard) attention implementation for EasyDeL.

This module provides a reference implementation of multi-head attention using
standard JAX operations. It serves as both a baseline for comparison with optimized
implementations and a fallback for platforms where specialized kernels are unavailable.

The vanilla attention implementation:
- Uses standard matrix multiplication and softmax operations
- Supports all standard attention features (masking, bias, dropout)
- Works on all platforms (TPU, GPU, CPU) without specialized kernels
- Provides full attention weights for inspection when needed
- Supports Grouped Query Attention (GQA) and Multi-Query Attention (MQA)

Key characteristics:
- Memory complexity: O(NÂ²) where N is sequence length
- Computation: Uses einsum for efficient batch matrix multiplication
- Flexibility: Supports various mask and bias shapes
- Compatibility: Works with any JAX backend without modification

This implementation is ideal for:
- Debugging and development
- Small sequence lengths where memory is not a constraint
- Platforms without optimized attention kernels
- Cases where attention weights need to be inspected

Example:
    >>> from easydel.layers.attention_operator import AttentionMetadata
    >>> from easydel.layers.attention_operator.modules import VanillaAttn
    >>>
    >>> metadata = AttentionMetadata(
    ...     runtime_dtype=jnp.float16,
    ...     runtime_softmax_dtype=jnp.float32,  # Higher precision for softmax
    ...     dropout_prob=0.1
    ... )
    >>> vanilla_attn = VanillaAttn(metadata)
    >>> output = vanilla_attn(query, key, value, mask=attention_mask)
    >>> attention_weights = output.attention_weights  # Available for inspection
"""

import typing as tp

import jax
from eformer.escale import with_sharding_constraint
from flax.nnx.nn.dtypes import promote_dtype
from jax import Array
from jax import numpy as jnp
from jax import random as jr
from jaxtyping import Bool, Float

from .._attention_impl import AttentionImpl, AttentionMetadata, AttentionOutput, AttentionRegistry


@AttentionRegistry.register
class VanillaAttn(AttentionImpl):
    """
    A standard, non-optimized implementation of multi-head attention.

    This implementation uses basic JAX operations like `jnp.einsum` and standard
    softmax. It serves as a reference implementation and a fallback for platforms
    where optimized kernels (like Flash Attention) are not available or desired.
    It supports features like attention bias, masking, dropout, and Grouped Query
    Attention (GQA)/Multi-Query Attention (MQA) via reshaping.

    Registered under the name "vanilla".
    """

    @classmethod
    def get_impl_name(cls) -> str | tuple[str]:
        """
        Returns the registered name of this attention implementation.

        Returns:
            The string "vanilla".
        """
        return "vanilla"

    def get_impl_metadata(self) -> AttentionMetadata:
        """
        Returns the metadata associated with this attention implementation instance.

        Returns:
            The `AttentionMetadata` provided during initialization.
        """
        return self.metadata

    @jax.named_scope("easydel-vanillaimpl-native-xla")
    def forward_native(
        self,
        q: Float[Array, "batch seq_len num_q_heads head_dim"],
        k: Float[Array, "batch kv_len num_kv_heads head_dim"],
        v: Float[Array, "batch kv_len num_kv_heads head_dim"],
        mask: Bool[Array, "batch 1 seq_len kv_len"] | None = None,
        bias: Float[Array, "batch num_heads seq_len kv_len"] | None = None,
        init_bias: tp.Callable[[], Float[Array, "batch num_heads seq_len kv_len"]] | None = None,
        deterministic: bool = True,  # Default to deterministic (no dropout)
        dropout_rng: jax.random.PRNGKey = None,
        softmax_aux: Float[Array, "..."] | None = None,
        **ignore,
    ) -> AttentionOutput:
        """
        Computes multi-head attention using standard JAX operations.

        Supports GQA/MQA by reshaping the query tensor to match the number of
        key/value heads. Applies scaling, optional bias/mask, softmax (potentially
        in float32), and optional dropout.

        Args:
            q: Query tensor with shape [batch, seq_len, num_q_heads, head_dim].
                The main input sequence to attend from.
            k: Key tensor with shape [batch, kv_len, num_kv_heads, head_dim].
                Keys for attention computation. May have fewer heads than queries (GQA/MQA).
            v: Value tensor with shape [batch, kv_len, num_kv_heads, head_dim].
                Values to aggregate based on attention weights.
            mask: Optional boolean mask with shape [batch, 1, seq_len, kv_len].
                True values indicate positions to attend to, False positions are masked.
                Used if `bias` is not provided.
            bias: Optional attention bias with shape [batch, num_heads, seq_len, kv_len].
                Additive bias applied to attention scores before softmax.
                Takes precedence over `mask`.
            init_bias: Optional callable that returns bias tensor.
                Used to lazily initialize bias if both mask and bias are None.
            deterministic: If True, disables dropout (default). If False, applies dropout.
            dropout_rng: JAX PRNG key for dropout. Required when deterministic=False
                and dropout_prob > 0 in metadata.
            softmax_aux: Optional auxiliary tensor for softmax computation.
            **ignore: Additional keyword arguments that are ignored.

        Returns:
            AttentionOutput containing:
                - attention_outputs: Float[Array, "batch seq_len num_q_heads head_dim"]
                  The attended representation.
                - attention_weights: Float[Array, "batch num_heads seq_len kv_len"] | None
                  The attention weights (if return_weights is True in metadata).

        Raises:
            NotImplementedError: If the bias head dimension cannot be reshaped correctly
                to match the query head structure for GQA/MQA.
        """

        sm_scale = self.metadata.softmax_scale
        sm_scale = sm_scale if sm_scale is not None else q.shape[-1] ** -0.5
        dtype = self.metadata.runtime_dtype
        softmax_dtype = self.metadata.runtime_softmax_dtype

        if softmax_dtype is None:
            softmax_dtype = jnp.float32

        model_mode = self.get_mode(q=q, BTHD=True)
        q_sharding, k_sharding, v_sharding, b_sharding, m_sharding, a_sharding = self.metadata.get_shardings(model_mode)
        if mask is None and bias is None and init_bias is not None:
            bias = init_bias()
        with self.metadata.mesh:
            if bias is None and mask is None and init_bias is not None:
                bias = init_bias()

            b, qs, qh, d = q.shape
            b, ks, kh, d = k.shape
            *_, vd = v.shape
            num_reps = qh // kh
            q = with_sharding_constraint(arr=q, sharding=q_sharding)
            k = with_sharding_constraint(arr=k, sharding=k_sharding)
            v = with_sharding_constraint(arr=v, sharding=v_sharding)

            bias = with_sharding_constraint(arr=bias, sharding=b_sharding) if bias is not None else bias
            mask = with_sharding_constraint(arr=mask, sharding=m_sharding) if mask is not None else mask

            q = jnp.reshape(q, (b, qs, kh, num_reps, d))
            q, k, v = promote_dtype((q, k, v), dtype=dtype)

            aw = jnp.einsum("bskhd,bmkd->bkhsm", q * sm_scale, k, optimize=True)

        if bias is not None:
            if bias.shape[1] == (kh * num_reps):
                bias = bias.reshape(b, kh, num_reps, qs, ks)
            elif bias.shape[1] == kh:
                bias = bias.reshape(b, kh, 1, qs, ks)
            elif bias.shape[1] == 1:
                bias = bias.reshape(b, 1, 1, qs, ks)
            else:
                raise NotImplementedError("bias heads wont match!")
            aw = jnp.add(aw, bias.astype(aw.dtype))

        elif mask is not None:
            if mask.dtype != jnp.bool_:
                mask = mask.astype(jnp.bool_)

            if mask.ndim == 4:
                if mask.shape[1] == 1:
                    mask = jnp.broadcast_to(mask, (b, kh, qs, ks))
                    mask = jnp.reshape(mask, (b, kh, 1, qs, ks))
                elif mask.shape[1] == kh:
                    mask = jnp.reshape(mask, (b, kh, 1, qs, ks))
                elif mask.shape[1] == (kh * num_reps):
                    mask = jnp.reshape(mask, (b, kh, num_reps, qs, ks))
                else:
                    mask = jnp.broadcast_to(mask[:, :1], (b, 1, qs, ks))
                    mask = jnp.reshape(mask, (b, 1, 1, qs, ks))
            elif mask.ndim == 3:
                mask = jnp.reshape(mask, (b, 1, 1, qs, ks))
            elif mask.ndim == 2:
                mask = jnp.reshape(mask, (b, 1, 1, 1, ks))
                mask = jnp.broadcast_to(mask, (b, 1, 1, qs, ks))
            else:
                raise ValueError(f"Unsupported mask shape: {mask.shape}")

            aw = jnp.where(mask, aw, jnp.finfo(aw.dtype).min)
        if softmax_aux is not None:
            if softmax_aux.ndim == 2:
                sinks = softmax_aux.reshape(1, kh, -1, 1, 1)
                sinks = jnp.broadcast_to(sinks, (b, kh, num_reps, qs, 1))
            elif softmax_aux.ndim == 1:
                sinks = softmax_aux.reshape(1, kh, -1, 1, 1)
                sinks = jnp.broadcast_to(sinks, (b, kh, num_reps, qs, 1))
            else:
                raise ValueError(f"Unsupported softmax_aux shape: {softmax_aux.shape}")
            combined_logits = jnp.concatenate([aw, sinks], axis=-1)
            combined_logits = combined_logits - jnp.max(combined_logits, axis=-1, keepdims=True)
            probs = jax.nn.softmax(combined_logits.astype(softmax_dtype), axis=-1).astype(dtype)
            aw = probs[..., :-1]
        else:
            aw = jax.nn.softmax(aw.astype(softmax_dtype), axis=-1).astype(dtype)

        dp = self.metadata.dropout_prob
        if not deterministic and dp > 0.0 and dropout_rng is not None:
            keep_prob = 1.0 - dp
            dropout_shape = tuple([1] * (k.ndim - 2)) + aw.shape[-2:]
            keep = jax.random.bernoulli(dropout_rng, keep_prob, dropout_shape)
            multiplier = keep.astype(dtype) / jnp.asarray(keep_prob, dtype=dtype)
            aw = aw * multiplier

        attention = jnp.einsum("bkhsm,bmkd->bskhd", aw, v, optimize=True).reshape(b, qs, qh, vd)

        return AttentionOutput(
            attention_weights=aw,
            attention_outputs=with_sharding_constraint(arr=attention, sharding=a_sharding),
        )

    def forward_gpu(self, *args, **kwargs) -> AttentionOutput:
        """GPU forward pass. Delegates to `forward_native`.

        Args:
            *args: Positional arguments for the attention calculation.
            **kwargs: Keyword arguments for the attention calculation.

        Returns:
            An `AttentionOutput` object containing the attention results.
        """
        return self.forward_cuda(*args, **kwargs)

    def forward_tpu(self, *args, **kwargs) -> AttentionOutput:
        """TPU forward pass. Delegates to `forward_native`.

        Args:
            *args: Positional arguments for the attention calculation.
            **kwargs: Keyword arguments for the attention calculation.

        Returns:
            An `AttentionOutput` object containing the attention results.
        """
        return self.forward_native(*args, **kwargs)

    def forward_cpu(self, *args, **kwargs) -> AttentionOutput:
        """CPU forward pass. Delegates to `forward_native`.

        Args:
            *args: Positional arguments for the attention calculation.
            **kwargs: Keyword arguments for the attention calculation.

        Returns:
            An `AttentionOutput` object containing the attention results.
        """
        return self.forward_native(*args, **kwargs)

    def forward_cuda(self, *args, **kwargs) -> AttentionOutput:
        """CUDA GPU forward pass. Delegates to `forward_native`.

        Args:
            *args: Positional arguments for the attention calculation.
            **kwargs: Keyword arguments for the attention calculation.

        Returns:
            An `AttentionOutput` object containing the attention results.
        """
        return self.forward_native(*args, **kwargs)

    def forward_rocm(self, *args, **kwargs) -> AttentionOutput:
        """ROCm GPU forward pass. Delegates to `forward_native`.

        Args:
            *args: Positional arguments for the attention calculation.
            **kwargs: Keyword arguments for the attention calculation.

        Returns:
            An `AttentionOutput` object containing the attention results.
        """
        return self.forward_native(*args, **kwargs)

    def __call__(
        self,
        q: Float[Array, "batch seq_len num_q_heads head_dim"],
        k: Float[Array, "batch kv_len num_kv_heads head_dim"],
        v: Float[Array, "batch kv_len num_kv_heads head_dim"],
        mask: Bool[Array, "batch 1 seq_len kv_len"] | None = None,
        bias: Float[Array, "batch num_heads seq_len kv_len"] | None = None,
        init_bias: tp.Callable[[], Float[Array, "batch num_heads seq_len kv_len"]] | None = None,
        deterministic: bool = True,
        dropout_rng: jax.random.PRNGKey = None,
        softmax_aux: Float[Array, "..."] | None = None,
        **ignore,
    ) -> AttentionOutput:
        """
        Executes the vanilla attention computation.

        Calls the appropriate backend-specific forward method via `super().__call__`.
        Since all backend methods delegate to `forward_native`, this effectively
        always runs the native JAX implementation.

        Args:
            q: Query tensor.
            k: Key tensor.
            v: Value tensor.
            mask: Optional attention mask.
            bias: Optional attention bias.
            init_bias: Optional callable to initialize bias.
            deterministic: If True, disables dropout.
            dropout_rng: JAX PRNG key for dropout if deterministic is False.
            **ignore: Additional ignored keyword arguments.

        Returns:
            An `AttentionOutput` object containing the attention results.
        """
        # Uses the BaseOperation.__call__ which reads self.metadata.backend for dispatch,
        # but all paths in VanillaAttn lead back to forward_native.
        return super().__call__(
            q=q,
            k=k,
            v=v,
            mask=mask,
            bias=bias,
            init_bias=init_bias,
            deterministic=deterministic,
            dropout_rng=dropout_rng,
            softmax_aux=softmax_aux,
            **ignore,
        )


if __name__ == "__main__":
    from easydel.infra import EasyDeLBaseConfig

    # Test cace when qkv might refer to mla
    b, qs, ks, qh, kh, d, vd = 1, 1024, 1024, 32, 8, 128, 128 + 64
    q = jr.normal(jr.key(0), (b, qs, qh, d), "f2")
    k = jr.normal(jr.key(1), (b, ks, kh, d), "f2")
    v = jr.normal(jr.key(2), (b, ks, kh, vd), "f2")
    a = jnp.astype(jr.randint(jr.key(3), (b, 1, qs, ks), 0, 4) > 2, "b1")

    metadata = AttentionMetadata(
        runtime_dtype=jnp.float16,
        runtime_softmax_dtype=jnp.float32,
        base_config=EasyDeLBaseConfig(),
        # backend="cpu",
    )

    attn = VanillaAttn(metadata)
    out = attn(q=q, k=k, v=v, mask=a)
